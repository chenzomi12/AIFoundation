1
00:00:00,000 --> 00:00:02,200
内容/录制:Z0MI酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,850 --> 00:00:03,566
hello大家好

3
00:00:03,566 --> 00:00:05,600
我是那个人在床上躺

4
00:00:05,733 --> 00:00:07,800
肥在心中减的ZOMI

5
00:00:10,133 --> 00:00:12,133
今天我们开一个完全新的大坑

6
00:00:12,133 --> 00:00:14,683
看一下整个大模型的推理的框架

7
00:00:14,683 --> 00:00:16,883
和整个大模型推理相关的知识

8
00:00:16,933 --> 00:00:17,850
那这个新坑

9
00:00:17,850 --> 00:00:19,283
可能会真的是有点大

10
00:00:19,366 --> 00:00:20,933
那我们在正式的内容之前

11
00:00:20,933 --> 00:00:22,650
我们要做一个简单的思考

12
00:00:23,133 --> 00:00:24,283
大模型的推理框架

13
00:00:24,283 --> 00:00:27,683
跟传统的推理框架有什么不一样

14
00:00:28,566 --> 00:00:29,133
那这里面

15
00:00:29,133 --> 00:00:31,050
ZOMI要跟大家一起去思考几个点了

16
00:00:31,050 --> 00:00:31,566
第一个点

17
00:00:31,566 --> 00:00:33,850
我能想到的就是大模型的推理框架

18
00:00:33,850 --> 00:00:35,933
它只推理大模型

19
00:00:36,200 --> 00:00:37,166
那大模型

20
00:00:37,166 --> 00:00:40,250
基本上就是聚焦在Transformer的这个架构

21
00:00:40,333 --> 00:00:42,483
所以说它肯定跟传统的推理引擎

22
00:00:42,483 --> 00:00:43,450
或者推理的模型

23
00:00:43,450 --> 00:00:45,450
或者推理框架是有不一样的

24
00:00:45,450 --> 00:00:47,133
传统的我需要支持CNN

25
00:00:47,133 --> 00:00:49,800
还有各种各样的GNN的模型

26
00:00:50,000 --> 00:00:50,283
第二个

27
00:00:50,283 --> 00:00:52,050
就是大模型的推理

28
00:00:52,050 --> 00:00:54,283
这么觉得还是应该包含服务化的内容

29
00:00:54,283 --> 00:00:55,250
因为大模型推理

30
00:00:55,250 --> 00:00:56,966
主要是在云端去使用的

31
00:00:56,966 --> 00:00:59,166
你说端侧去跑一个大模型现在来说

32
00:00:59,166 --> 00:01:00,450
还是相对比较少的

33
00:01:00,450 --> 00:01:01,733
所以大模型的推理框架

34
00:01:01,733 --> 00:01:02,966
跟传统的推理框架

35
00:01:03,050 --> 00:01:04,166
传统的推理框架

36
00:01:04,166 --> 00:01:05,566
其实是主要包含

37
00:01:05,566 --> 00:01:07,766
或者主要指的是推理引擎

38
00:01:08,166 --> 00:01:10,566
那大模型的推理框架或者推理引擎

39
00:01:10,566 --> 00:01:11,566
主要包含

40
00:01:11,566 --> 00:01:12,933
可能会大模型的推理引擎

41
00:01:12,933 --> 00:01:14,533
和大模型的服务化内容

42
00:01:14,800 --> 00:01:16,683
那还有没有其他区别

43
00:01:16,766 --> 00:01:18,250
既然有这两个特质

44
00:01:18,250 --> 00:01:21,333
那整个在架构上又有哪些区别

45
00:01:21,366 --> 00:01:23,366
希望大家一起去带着这个问题

46
00:01:23,366 --> 00:01:25,366
思考未来的一些内容

47
00:01:26,766 --> 00:01:28,566
那我们现在来到了正式的

48
00:01:28,566 --> 00:01:29,050
看一下

49
00:01:29,050 --> 00:01:31,050
我们在后面的所有的序列里面

50
00:01:31,050 --> 00:01:32,200
去跟大家分享哪

51
00:01:32,200 --> 00:01:33,333
些知识点

52
00:01:33,333 --> 00:01:34,200
首先这里面

53
00:01:34,200 --> 00:01:37,333
分开6个知识点去跟大家去汇报的

54
00:01:37,333 --> 00:01:38,650
后面那首先第一个

55
00:01:38,650 --> 00:01:40,450
就是大模型的推理框架

56
00:01:40,450 --> 00:01:42,800
我们会真正的去看看大模型推理框架

57
00:01:42,800 --> 00:01:43,966
跟推理引擎

58
00:01:44,133 --> 00:01:46,733
或传统的推理引擎有哪些区别

59
00:01:46,766 --> 00:01:48,800
也就是做一个vs的对比

60
00:01:48,966 --> 00:01:50,850
近日我们可能会去深入的看一下

61
00:01:50,850 --> 00:01:53,450
大模型的推理加速的相关的手段

62
00:01:53,450 --> 00:01:54,600
和相关的算法

63
00:01:54,683 --> 00:01:57,850
有了对大模型的一些推理加速之外

64
00:01:57,850 --> 00:02:00,600
我们还需要看一下整个框架的调度的

65
00:02:00,600 --> 00:02:01,933
因为现在大模型推理

66
00:02:01,933 --> 00:02:04,000
主要是分为perfill和decode阶段

67
00:02:04,000 --> 00:02:05,083
那整体的调度

68
00:02:05,083 --> 00:02:06,533
又分为containers back

69
00:02:06,533 --> 00:02:08,800
还有那个投机推理相关的内容

70
00:02:08,800 --> 00:02:09,600
所以我们去看一下

71
00:02:09,600 --> 00:02:11,800
整体的调度有什么区别

72
00:02:12,166 --> 00:02:12,600
接着

73
00:02:12,600 --> 00:02:15,283
因为现在的模型的序列越来越长了

74
00:02:15,283 --> 00:02:17,566
所以我们看一下整个长序列的推理

75
00:02:17,566 --> 00:02:18,400
现在推理

76
00:02:18,400 --> 00:02:19,250
类似于Kimi

77
00:02:19,250 --> 00:02:21,366
一开始就推出了长序列的推理

78
00:02:21,366 --> 00:02:24,083
直接把一本书好多的摘要丢进去

79
00:02:24,083 --> 00:02:25,166
让大模型去学

80
00:02:25,166 --> 00:02:26,000
我们直接去问

81
00:02:26,000 --> 00:02:27,566
大模型相关的内容就可以了

82
00:02:27,966 --> 00:02:29,683
那有了这一基本的介绍之后

83
00:02:29,683 --> 00:02:32,083
我们看一下最终的输出采样

84
00:02:32,083 --> 00:02:33,683
因为在整个大模型的输出

85
00:02:33,683 --> 00:02:36,250
说实话它只是一个序列或者embedding 层

86
00:02:36,250 --> 00:02:37,683
最后输出的一些TOKEN

87
00:02:37,683 --> 00:02:39,650
其实TOKEN意味着的是什么

88
00:02:39,650 --> 00:02:41,000
它到底对应的是哪些字

89
00:02:41,000 --> 00:02:43,166
我们会有一个输出的采样

90
00:02:43,166 --> 00:02:45,966
最终才能变成我们能够看的内容

91
00:02:46,283 --> 00:02:47,200
那在第六个内容

92
00:02:47,200 --> 00:02:49,050
我们可能会引入一些额外的支持

93
00:02:49,050 --> 00:02:52,050
大模型的压缩量化相关的知识点

94
00:02:52,050 --> 00:02:52,650
那接下来

95
00:02:52,650 --> 00:02:53,566
我们逐个的打开

96
00:02:53,566 --> 00:02:56,166
看一下我们后面的分开多少个视频跟

97
00:02:56,166 --> 00:02:57,600
大家去讲解的

98
00:02:57,600 --> 00:02:58,333
那这一期

99
00:02:58,333 --> 00:03:00,650
其实主要是做一个introduction

100
00:03:00,650 --> 00:03:01,600
summary的阶段

101
00:03:01,600 --> 00:03:04,000
我们后面会分开每一个视频

102
00:03:04,000 --> 00:03:05,733
全面的跟大家去探讨一下

103
00:03:05,733 --> 00:03:07,083
大模型的推理框架的

104
00:03:07,083 --> 00:03:07,883
首先

105
00:03:07,933 --> 00:03:09,933
我们来看一下整个大模型的推理流程

106
00:03:09,933 --> 00:03:11,083
因为大模型推理

107
00:03:11,083 --> 00:03:12,133
主要是分开两个阶段

108
00:03:12,133 --> 00:03:12,566
一个perfill

109
00:03:12,566 --> 00:03:13,566
一个decoder阶段嘛

110
00:03:13,566 --> 00:03:14,650
那有了这个阶段之后

111
00:03:14,650 --> 00:03:15,766
我们就正式的引入

112
00:03:15,766 --> 00:03:17,766
整个大模型的推理框架的概述

113
00:03:17,766 --> 00:03:19,366
包括现在的SGLang

114
00:03:19,366 --> 00:03:20,600
还有vLLM

115
00:03:20,733 --> 00:03:21,166
还有

116
00:03:21,166 --> 00:03:23,650
LM deploy等相关的开源的推理引擎

117
00:03:23,650 --> 00:03:24,933
当然了还有华为的mindIE

118
00:03:24,933 --> 00:03:27,000
相关的内容

119
00:03:27,000 --> 00:03:28,533
接着我们会以其中一个

120
00:03:28,533 --> 00:03:30,850
现在特别的火的推理引擎

121
00:03:30,850 --> 00:03:32,850
或者大目前推理框架作为介绍

122
00:03:32,850 --> 00:03:33,533
就看一下

123
00:03:33,533 --> 00:03:35,966
vLLM的一个整体的架构的剖析

124
00:03:35,966 --> 00:03:36,400
这里面

125
00:03:36,400 --> 00:03:38,333
可能分开3-4个视频

126
00:03:38,333 --> 00:03:39,800
跟大家详细的介绍

127
00:03:39,883 --> 00:03:42,733
vLLM这个架构到底有哪些内容

128
00:03:43,050 --> 00:03:43,483
那接着

129
00:03:43,483 --> 00:03:46,050
我们看一下整个推理的性能的指标

130
00:03:46,050 --> 00:03:47,850
特别是大模型推理的性能指标

131
00:03:47,850 --> 00:03:49,800
跟传统的有哪些区别

132
00:03:49,800 --> 00:03:53,050
我们首Token和后面的每个decoder阶段

133
00:03:53,166 --> 00:03:55,050
到底是怎么去核算的

134
00:03:56,050 --> 00:03:57,883
有了整个大模型的推理框架的

135
00:03:57,883 --> 00:03:58,883
一个整体的概述

136
00:03:58,883 --> 00:04:00,883
之后换了相关性的认知之后

137
00:04:00,883 --> 00:04:02,966
我们看一下整个大模型的推理的加速

138
00:04:03,050 --> 00:04:03,800
那推理加速

139
00:04:03,800 --> 00:04:04,933
其实最核心

140
00:04:04,933 --> 00:04:06,366
最核心或者最原始的

141
00:04:06,366 --> 00:04:09,766
肯定需要一个KV-Cache的一个过程和阶段

142
00:04:09,766 --> 00:04:10,883
因为大模型推理

143
00:04:10,883 --> 00:04:13,883
它是非常重显存的一个过程了

144
00:04:13,883 --> 00:04:14,800
在decoder阶段

145
00:04:15,050 --> 00:04:15,400
那接着

146
00:04:15,400 --> 00:04:18,000
我们就会深入的打开其中的几个算法

147
00:04:18,000 --> 00:04:19,500
第一个就是Paged Attention

148
00:04:19,500 --> 00:04:20,800
第二个是flash Attention

149
00:04:20,800 --> 00:04:22,000
还有LLm in flash

150
00:04:22,000 --> 00:04:24,533
还有streamLLM相关的算法了

151
00:04:24,533 --> 00:04:25,683
原理可能

152
00:04:25,683 --> 00:04:26,250
每一个算法

153
00:04:26,250 --> 00:04:28,683
我们分开一期视频跟大家去介绍

154
00:04:28,683 --> 00:04:30,250
那有些可能我们会分开

155
00:04:30,250 --> 00:04:32,250
或者把它两个合成一个

156
00:04:32,250 --> 00:04:33,650
跟大家一起去介绍的

157
00:04:33,650 --> 00:04:35,366
方便我们更好的去理解

158
00:04:35,366 --> 00:04:37,166
大模型的推理加速

159
00:04:37,366 --> 00:04:38,566
到底用哪些算法

160
00:04:38,966 --> 00:04:40,083
了解完推理加速之后

161
00:04:40,083 --> 00:04:42,133
我们还需要看一下调度的加速

162
00:04:42,133 --> 00:04:43,250
因为现在大模型

163
00:04:43,250 --> 00:04:43,966
最推崇的

164
00:04:43,966 --> 00:04:44,966
或者最容易用的

165
00:04:44,966 --> 00:04:46,800
就是PD分裂的整体加构

166
00:04:46,800 --> 00:04:48,250
这是微软发布的一篇文章

167
00:04:48,250 --> 00:04:49,733
还有现在的Kimi

168
00:04:49,733 --> 00:04:51,283
就是我们的月之暗面

169
00:04:51,400 --> 00:04:52,850
也在大规模的区部署

170
00:04:52,883 --> 00:04:54,733
PD分离的整体的架构

171
00:04:54,733 --> 00:04:56,516
另外的话我们还有一些Chunked Prefill

172
00:04:56,516 --> 00:04:57,883
还有Continuous Batching 

173
00:04:57,883 --> 00:04:58,850
还有Prefix Caching

174
00:04:58,850 --> 00:05:00,883
还有FastChat这个框架里面的

175
00:05:00,883 --> 00:05:04,800
做一些均衡的负载相关的调度

176
00:05:05,050 --> 00:05:06,166
刚才的推理加速

177
00:05:06,166 --> 00:05:08,133
更多的是算子层和执行层面的

178
00:05:08,133 --> 00:05:08,850
现在这层

179
00:05:08,850 --> 00:05:10,000
更多是在部署层面

180
00:05:10,000 --> 00:05:12,800
和上层一点的相关的调度的加速

181
00:05:12,933 --> 00:05:14,083
了解完两个加速手段

182
00:05:14,083 --> 00:05:16,566
我们看一下整个大模型的长序列

183
00:05:16,683 --> 00:05:17,483
LongLoRA 

184
00:05:17,483 --> 00:05:17,766
FastGen  

185
00:05:17,766 --> 00:05:18,283
Ring Attention 

186
00:05:18,283 --> 00:05:20,566
还有Homer整体的分层架构原理

187
00:05:20,566 --> 00:05:22,533
看下整个长序列的推理

188
00:05:22,533 --> 00:05:24,566
到底又有哪些新的手段

189
00:05:24,966 --> 00:05:25,933
了解完这一切之后

190
00:05:25,933 --> 00:05:28,800
我们看一下真正的大模型的输出

191
00:05:28,850 --> 00:05:29,966
那embedding层之后

192
00:05:29,966 --> 00:05:32,133
我们会输出一系列的一些TOKEN

193
00:05:32,133 --> 00:05:33,600
那这些TOKEN是我们不可见的

194
00:05:33,600 --> 00:05:35,566
或者它不是真正的一个个字

195
00:05:35,600 --> 00:05:36,966
怎么把它变成具体的字

196
00:05:36,966 --> 00:05:38,400
怎么变成一句话

197
00:05:38,400 --> 00:05:38,966
这个时候

198
00:05:38,966 --> 00:05:41,050
就非常依赖于我们的采样的算法

199
00:05:41,050 --> 00:05:41,416
Greedy

200
00:05:41,416 --> 00:05:42,050
Stochastic

201
00:05:42,050 --> 00:05:45,250
还有Beam search相关的一些采样的算法

202
00:05:45,250 --> 00:05:46,400
当然了除了这些采样算法

203
00:05:46,400 --> 00:05:47,733
我们还有Speculative Decoding

204
00:05:47,733 --> 00:05:48,966
还有Lookahead Decoding

205
00:05:48,966 --> 00:05:50,083
还有Parallel Decoding

206
00:05:50,083 --> 00:05:51,600
当然啦现在哦

207
00:05:51,600 --> 00:05:52,733
现在来说

208
00:05:52,766 --> 00:05:53,683
Moe的架构

209
00:05:53,683 --> 00:05:54,483
是非常火的

210
00:05:54,483 --> 00:05:55,650
特别是deep seek

211
00:05:55,650 --> 00:05:56,766
MOE出现之后

212
00:05:56,766 --> 00:05:58,366
我们看一下MOE的推理采样

213
00:05:58,366 --> 00:06:00,133
还有专家是怎么选择的

214
00:06:00,133 --> 00:06:01,683
最后还会有个早退出

215
00:06:01,683 --> 00:06:04,050
还有级联的相关的推理输出

216
00:06:04,733 --> 00:06:05,733
了解完这一切之后

217
00:06:05,733 --> 00:06:08,483
我们最后来一个大模型的压缩

218
00:06:08,483 --> 00:06:12,450
现在英伟达的B200还是H20

219
00:06:12,450 --> 00:06:16,133
H40都开始慢慢的推更低精度的推理

220
00:06:16,133 --> 00:06:19,400
当然我们还会有一些量化相关的技术

221
00:06:19,483 --> 00:06:21,200
但是剪枝蒸馏在这里面

222
00:06:21,200 --> 00:06:22,800
其实越来越少了

223
00:06:23,166 --> 00:06:25,133
我们在后面的一系列的视频里面

224
00:06:25,133 --> 00:06:26,850
就会真正的去跟大家

225
00:06:26,850 --> 00:06:29,600
重点的打开大模型推理相关的内容了

226
00:06:29,600 --> 00:06:30,450
这里面的PPT

227
00:06:30,450 --> 00:06:31,650
ZOMI已经写的差不多了

228
00:06:31,650 --> 00:06:32,450
所以剩下内容

229
00:06:32,450 --> 00:06:34,733
就是录课剪辑

230
00:06:34,850 --> 00:06:36,050
今天先到这里

231
00:06:36,050 --> 00:06:36,600
谢谢各位

232
00:06:36,600 --> 00:06:37,400
拜了个拜

