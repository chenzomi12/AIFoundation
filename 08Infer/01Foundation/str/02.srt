1
00:00:00,000 --> 00:00:01,850
内容/录制:Z0MI酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,283 --> 00:00:05,166
我们说的搞钱其实就去打工

3
00:00:05,283 --> 00:00:08,000
我们说的努力其实就是去上班

4
00:00:16,133 --> 00:00:17,800
大家好我是一边努力

5
00:00:17,800 --> 00:00:19,933
一边打工一边赚钱的ZOMI

6
00:00:20,283 --> 00:00:20,650
今天

7
00:00:20,650 --> 00:00:22,933
我们来到了整个大模型推理的框架

8
00:00:22,933 --> 00:00:24,133
看一下总体

9
00:00:24,133 --> 00:00:25,966
现在大模型推理框架有哪一些

10
00:00:26,483 --> 00:00:28,333
那在整个大模型推理框架里面

11
00:00:28,333 --> 00:00:30,083
我们现在来到了第二课内容

12
00:00:30,083 --> 00:00:32,250
看一下推理框架的整体的概述

13
00:00:32,366 --> 00:00:33,650
那在这一节视频里面

14
00:00:33,650 --> 00:00:35,566
主要是跟大家分享几个知识点

15
00:00:35,766 --> 00:00:35,966
第一个

16
00:00:35,966 --> 00:00:38,650
就是看一下大模型业界的推理框架

17
00:00:38,650 --> 00:00:40,050
或者推理引擎有哪些

18
00:00:40,133 --> 00:00:42,650
接着我们分开几个来去横向的对比

19
00:00:42,650 --> 00:00:44,333
第一个就是Huggingface的TGI啦

20
00:00:44,533 --> 00:00:46,000
vLLM sGLang

21
00:00:46,000 --> 00:00:47,533
还有LMDeploy

22
00:00:47,533 --> 00:00:48,133
那这几个

23
00:00:48,133 --> 00:00:49,400
都是现在开源

24
00:00:49,400 --> 00:00:50,450
还有一些没有开源

25
00:00:50,450 --> 00:00:52,166
ZOMI就没有展开了

26
00:00:52,200 --> 00:00:53,200
那接着我们看一下

27
00:00:53,200 --> 00:00:55,083
这几个框架的一个性能的对比

28
00:00:55,083 --> 00:00:57,733
之后做一个简单的总结和思考

29
00:01:00,366 --> 00:01:01,683
正式开始今天内容之前

30
00:01:01,683 --> 00:01:03,966
我觉得还是有一些灵魂的思考

31
00:01:04,000 --> 00:01:06,566
到底是叫大模型推理框架

32
00:01:06,566 --> 00:01:09,083
还是叫大模型的推理引擎

33
00:01:09,083 --> 00:01:11,166
还是叫大模型的推理服务

34
00:01:11,166 --> 00:01:14,050
几个概念应该怎么去划分

35
00:01:14,050 --> 00:01:15,800
这个ZOMI有时候也会搞混

36
00:01:15,800 --> 00:01:16,133
不过

37
00:01:16,133 --> 00:01:18,200
随着ZOMI对整个大模型推理相关

38
00:01:18,200 --> 00:01:19,400
加速的内容

39
00:01:19,600 --> 00:01:20,200
越来越熟悉

40
00:01:20,200 --> 00:01:22,800
之后我觉得大模型的推理引擎

41
00:01:22,800 --> 00:01:24,366
其实跟传统推理引擎一样

42
00:01:24,366 --> 00:01:27,050
都是指那个LLM的back and后面

43
00:01:27,050 --> 00:01:28,533
做一些加速的手段

44
00:01:28,533 --> 00:01:30,250
那大模型的推理服务话

45
00:01:30,250 --> 00:01:31,533
其实类似于Tritan

46
00:01:31,533 --> 00:01:34,283
那种包括我们的对外的推理服务

47
00:01:34,283 --> 00:01:36,250
还有我们看到的Web界面

48
00:01:36,450 --> 00:01:38,566
其实这两个加起来

49
00:01:38,566 --> 00:01:39,250
ZOMI觉得

50
00:01:39,250 --> 00:01:41,683
才是真正的大模型的推理框架

51
00:01:41,683 --> 00:01:44,166
所以我们说到的大模型的推理框架

52
00:01:44,166 --> 00:01:46,200
它的意义上或者更加泛化一点

53
00:01:46,200 --> 00:01:48,133
它包括推理引擎和推理服务

54
00:01:48,133 --> 00:01:48,766
那后面

55
00:01:48,766 --> 00:01:50,883
我们其实有一些推理框架

56
00:01:51,166 --> 00:01:53,333
它可能只是单点的只推理引擎

57
00:01:53,333 --> 00:01:53,600
当然了

58
00:01:53,600 --> 00:01:55,883
有一些它可能会包括推理服务在里面

59
00:01:55,883 --> 00:01:58,366
例如vLLM它就包含推理服务了

60
00:01:58,366 --> 00:01:59,366
但是LM deploy

61
00:01:59,366 --> 00:02:01,650
它可能更多是聚焦于推理引擎

62
00:02:01,650 --> 00:02:03,883
包括C++内层相关的内容

63
00:02:03,883 --> 00:02:04,400
那但是

64
00:02:04,400 --> 00:02:05,533
我们现在笼统

65
00:02:05,533 --> 00:02:08,400
都用推理框架来去概述

66
00:02:09,683 --> 00:02:11,483
那第二个我们要灵魂思考

67
00:02:11,483 --> 00:02:12,050
就是

68
00:02:12,050 --> 00:02:15,683
大模型推理的加速的目的是什么

69
00:02:15,683 --> 00:02:17,000
就看一下真正的目

70
00:02:17,000 --> 00:02:18,333
我们有了这个目

71
00:02:18,333 --> 00:02:19,683
才能够更好的牵引我们

72
00:02:19,683 --> 00:02:21,133
去设计一个好

73
00:02:21,133 --> 00:02:23,450
大模型的推理框架出来

74
00:02:23,450 --> 00:02:25,566
所以我们一定要搞清楚什么是目

75
00:02:25,566 --> 00:02:26,133
那ZOMI

76
00:02:26,133 --> 00:02:26,650
在这里面

77
00:02:26,650 --> 00:02:28,966
就总结了两个词

78
00:02:29,800 --> 00:02:30,283
就很简单了

79
00:02:30,283 --> 00:02:31,850
就大模型的推理的加速

80
00:02:31,850 --> 00:02:32,566
真正的目

81
00:02:32,566 --> 00:02:35,733
我们要做到高吞吐和低延迟

82
00:02:35,733 --> 00:02:38,366
这两个有时候可能是相悖

83
00:02:38,366 --> 00:02:39,200
这里面

84
00:02:39,200 --> 00:02:42,050
有点像昨天ZOMI刷抖音刷到

85
00:02:42,050 --> 00:02:43,116
相爱跟分开

86
00:02:43,116 --> 00:02:44,200
有可能是相悖

87
00:02:44,200 --> 00:02:46,650
那高吞吐跟低延迟有可能是相悖

88
00:02:46,650 --> 00:02:48,200
那所谓的高吞吐

89
00:02:48,200 --> 00:02:50,116
这是指我们的整个推理情况

90
00:02:50,116 --> 00:02:51,400
整个推理框架

91
00:02:51,400 --> 00:02:53,166
有很多的人去访问

92
00:02:53,166 --> 00:02:55,283
我们需要做到非常高的吞吐

93
00:02:55,283 --> 00:02:57,316
能够处理非常多的数据

94
00:02:57,566 --> 00:02:59,483
但是你处理数据多的时候

95
00:02:59,483 --> 00:03:01,483
你可能就会导致延迟增加了

96
00:03:01,483 --> 00:03:01,850
但是

97
00:03:01,850 --> 00:03:03,600
我们一般来说我不会

98
00:03:03,600 --> 00:03:04,766
就是站在用户角度

99
00:03:04,766 --> 00:03:06,250
我输入一个请求

100
00:03:06,250 --> 00:03:09,000
我希望马上得到一个答案或者答复

101
00:03:09,000 --> 00:03:09,966
我说一个Prompt

102
00:03:09,966 --> 00:03:11,400
我希望这个大模型推理框架

103
00:03:11,400 --> 00:03:13,116
能够马上给我一个答复

104
00:03:13,116 --> 00:03:13,650
那这个

105
00:03:13,650 --> 00:03:15,116
我就希望低延迟

106
00:03:15,166 --> 00:03:16,200
延迟做的更

107
00:03:16,200 --> 00:03:17,166
越低越好

108
00:03:17,166 --> 00:03:18,850
但是在一个推理框架角度

109
00:03:18,850 --> 00:03:21,683
我希望把多个用户的请求放在一起

110
00:03:21,916 --> 00:03:23,083
把吞吐做大

111
00:03:23,083 --> 00:03:24,566
然后统一返回

112
00:03:24,566 --> 00:03:26,650
所以我们既希望高吞吐

113
00:03:26,650 --> 00:03:28,000
也希望低延迟

114
00:03:28,000 --> 00:03:28,966
这个就是我们做

115
00:03:28,966 --> 00:03:30,200
整个大模型推理框架

116
00:03:30,200 --> 00:03:31,283
一个目的了

117
00:03:31,483 --> 00:03:32,366
了解完这个之后

118
00:03:32,366 --> 00:03:36,450
我们开始真正的去了解后面的内容

119
00:03:36,450 --> 00:03:38,166
我们现在才来到了第一个内容

120
00:03:38,166 --> 00:03:41,450
看业界有哪些大模型的推理框架

121
00:03:42,050 --> 00:03:43,316
那说实话这一期视频

122
00:03:43,316 --> 00:03:44,883
还是相对的比较火

123
00:03:44,883 --> 00:03:45,883
我们现在来说

124
00:03:45,883 --> 00:03:47,650
真正的大模型的推理引擎

125
00:03:47,650 --> 00:03:48,716
从2024年来说

126
00:03:48,716 --> 00:03:50,516
是非常的粗糙

127
00:03:50,766 --> 00:03:52,283
但是现在至少能用了

128
00:03:52,283 --> 00:03:53,516
到了现在为止

129
00:03:53,516 --> 00:03:55,000
或者到了2024年年底了

130
00:03:55,000 --> 00:03:57,883
包括我们即将迈向2025年了

131
00:03:57,883 --> 00:03:59,450
现在的大模型推理框架

132
00:03:59,450 --> 00:04:01,316
已经基本上能够实现好用了

133
00:04:01,366 --> 00:04:03,116
稍微有那么点粗糙

134
00:04:03,116 --> 00:04:03,916
那基本上

135
00:04:03,916 --> 00:04:05,166
还是这个阶段

136
00:04:05,166 --> 00:04:06,716
能用好用粗糙

137
00:04:06,766 --> 00:04:08,400
这个阶段去不断的发展

138
00:04:08,400 --> 00:04:10,400
不过现在整个大模型推理框架

139
00:04:10,400 --> 00:04:12,483
也是处于一个高速的发展阶段

140
00:04:13,250 --> 00:04:14,483
ZOMI老师你好

141
00:04:14,483 --> 00:04:15,566
我想问一下

142
00:04:15,566 --> 00:04:18,050
为什么你说从长远角度来看

143
00:04:18,200 --> 00:04:21,083
今天的开源大模型的框架

144
00:04:21,083 --> 00:04:22,716
实际上比较粗糙

145
00:04:23,683 --> 00:04:26,366
哎小新的提的这个问题还蛮好

146
00:04:26,366 --> 00:04:26,800
蛮有意思

147
00:04:26,800 --> 00:04:28,250
我们简单的分开几个点

148
00:04:28,250 --> 00:04:29,850
跟大家去分享

149
00:04:29,883 --> 00:04:32,800
首先整个大目前推理框架

150
00:04:32,800 --> 00:04:34,283
整体来说社区

151
00:04:34,283 --> 00:04:35,083
现在是充满活力

152
00:04:35,083 --> 00:04:37,716
在快速的探索和发展过程当中

153
00:04:38,083 --> 00:04:40,400
但是我们现在的大模型相关的新技术

154
00:04:40,400 --> 00:04:41,800
发展的非常的快

155
00:04:41,800 --> 00:04:43,050
所以我们整个推理引擎

156
00:04:43,050 --> 00:04:45,516
承受了太多他不应该承受的内容

157
00:04:45,516 --> 00:04:47,516
或者我们整个推理框架承受了很多

158
00:04:47,516 --> 00:04:50,000
他不应该承受的相关的特性

159
00:04:50,166 --> 00:04:52,200
所以它整体来说相对粗糙

160
00:04:52,250 --> 00:04:54,800
例如PD分离也是24年的时候

161
00:04:54,800 --> 00:04:55,516
刚提出没多久

162
00:04:55,516 --> 00:04:56,083
然后现在

163
00:04:56,083 --> 00:04:57,516
其实很多推理框架

164
00:04:57,516 --> 00:04:58,883
已经开始慢慢的落地了

165
00:04:58,883 --> 00:04:59,516
另外的话

166
00:04:59,516 --> 00:05:01,050
整个AI的推理框架

167
00:05:01,050 --> 00:05:03,766
要支持非常多不同的新的模型

168
00:05:03,766 --> 00:05:05,083
和新的硬件了

169
00:05:05,083 --> 00:05:06,916
那现在除了大语言模型以外

170
00:05:06,916 --> 00:05:08,650
我们还会迎来了CV大模型

171
00:05:08,650 --> 00:05:09,483
视频大模型

172
00:05:09,483 --> 00:05:10,450
多模态大模型

173
00:05:10,450 --> 00:05:12,566
还有各种各样的大模型

174
00:05:12,766 --> 00:05:13,566
那另外的话

175
00:05:13,566 --> 00:05:14,316
整体来说

176
00:05:14,316 --> 00:05:16,483
除了我们做一个大模型推理以外

177
00:05:16,483 --> 00:05:18,283
我们为什么叫它一个框架

178
00:05:18,283 --> 00:05:19,916
不简单的叫一个引擎

179
00:05:19,916 --> 00:05:22,250
因为这里面已经涉及到模型结构

180
00:05:22,283 --> 00:05:23,650
具体的计算加速策略

181
00:05:23,650 --> 00:05:24,200
调制策略

182
00:05:24,200 --> 00:05:24,883
存储策略

183
00:05:24,883 --> 00:05:28,116
和硬件加速等相关非常多的内容

184
00:05:28,116 --> 00:05:30,283
所以我们从场景的角度来去看

185
00:05:30,283 --> 00:05:33,483
现今的一个开源大模型推理框架

186
00:05:33,716 --> 00:05:35,116
相对来说比较粗糙

187
00:05:35,116 --> 00:05:36,200
但是能用

188
00:05:37,083 --> 00:05:38,116
那接着我们看一下

189
00:05:38,116 --> 00:05:40,850
整个现在的业界的主流的推理框架

190
00:05:40,850 --> 00:05:42,450
首先第一个就是TGI

191
00:05:42,450 --> 00:05:45,916
TGI强音称叫做text generation inference

192
00:05:46,083 --> 00:05:46,800
而这里面

193
00:05:46,800 --> 00:05:48,450
很重要的就是Huggingface自己推出

194
00:05:48,450 --> 00:05:48,883
不过

195
00:05:48,883 --> 00:05:50,850
说实话业界用的还是相对比较少

196
00:05:50,850 --> 00:05:52,000
大家用的更多的是

197
00:05:52,000 --> 00:05:53,516
Huggingface里面的transformer

198
00:05:53,516 --> 00:05:55,000
还有其他的一些库

199
00:05:55,283 --> 00:05:57,916
那另外的话还有那个最近特别火

200
00:05:57,916 --> 00:05:59,483
vLLM那vLLM

201
00:05:59,483 --> 00:06:01,516
其实我们在做一客户的咨询当中

202
00:06:01,516 --> 00:06:03,883
发现好多客户都在用vLLM

203
00:06:03,883 --> 00:06:06,400
包括那最近一直还没开源

204
00:06:06,400 --> 00:06:07,366
一个mooncake

205
00:06:07,366 --> 00:06:10,316
其实也是基于vLLM进行一个魔改

206
00:06:10,600 --> 00:06:11,116
月之暗面

207
00:06:11,116 --> 00:06:14,000
也是基于这个来进行磨改

208
00:06:14,000 --> 00:06:15,050
所以相关的内容

209
00:06:15,050 --> 00:06:16,166
也是比较相似

210
00:06:16,166 --> 00:06:17,516
另外还有一个同门师兄弟

211
00:06:17,516 --> 00:06:18,683
就是sGLang了

212
00:06:18,683 --> 00:06:21,366
也是对大模型进行一个推理框架

213
00:06:21,366 --> 00:06:23,250
然后也在学校里面进开源

214
00:06:23,250 --> 00:06:23,883
另外的话

215
00:06:23,883 --> 00:06:25,200
上智院

216
00:06:25,200 --> 00:06:26,916
上智院确实也还存在(勘误)

217
00:06:26,916 --> 00:06:28,800
大家不要觉得商汤已经不在了(勘误)

218
00:06:28,800 --> 00:06:29,566
那上智院里面

219
00:06:29,566 --> 00:06:31,400
又推出了自己的LLM deploy

220
00:06:31,400 --> 00:06:31,966
那ZOMI

221
00:06:31,966 --> 00:06:33,916
有幸认识是LLM deploy的一个负责人

222
00:06:33,916 --> 00:06:35,400
不过ZOMI跟他们的思路

223
00:06:35,400 --> 00:06:37,200
还有很多不一样的点

224
00:06:37,200 --> 00:06:38,766
还值得去探讨

225
00:06:39,200 --> 00:06:41,250
那厂商自配的一个推引引擎

226
00:06:41,250 --> 00:06:43,366
因为每个厂商都会有自己的硬件嘛

227
00:06:43,366 --> 00:06:45,366
然后他们其实也是希望推出自己

228
00:06:45,366 --> 00:06:46,716
大模型的推理框架

229
00:06:46,716 --> 00:06:47,116
那例如

230
00:06:47,116 --> 00:06:49,250
我们看一下整个华为昇腾里面

231
00:06:49,250 --> 00:06:50,966
具有整个MindIE

232
00:06:50,966 --> 00:06:51,650
然后英伟达

233
00:06:51,650 --> 00:06:54,450
就会推出自己的TensorRT-LLM

234
00:06:54,450 --> 00:06:55,483
这里面打错了

235
00:06:55,683 --> 00:06:57,166
那真正的TensorRT

236
00:06:57,166 --> 00:06:59,250
以前的跟TensorRT-LLM

237
00:06:59,250 --> 00:07:00,283
它是两个东西

238
00:07:00,283 --> 00:07:01,966
两个不同的团队去开发

239
00:07:01,966 --> 00:07:02,966
我们现在

240
00:07:02,966 --> 00:07:04,883
因为这个TensorRT是英伟达

241
00:07:04,883 --> 00:07:07,000
这个是华为昇腾自己

242
00:07:07,000 --> 00:07:07,600
那我们现在

243
00:07:07,600 --> 00:07:10,083
简单打开华为的昇腾的官网

244
00:07:10,083 --> 00:07:11,650
看一下MindIE

245
00:07:11,650 --> 00:07:13,000
这里面有哪些内容

246
00:07:13,000 --> 00:07:15,166
这里面说MindIE是面向推理执行

247
00:07:15,166 --> 00:07:16,166
不过实际上

248
00:07:16,166 --> 00:07:18,600
MindIE主要是针对大模型业务

249
00:07:18,600 --> 00:07:21,650
那整个MindIE虽然叫推理引擎

250
00:07:21,650 --> 00:07:22,883
但是ZOMI觉得更多

251
00:07:22,883 --> 00:07:24,516
应该叫做推理框架

252
00:07:24,516 --> 00:07:25,316
为什么这么说

253
00:07:25,316 --> 00:07:26,000
因为这里面

254
00:07:26,000 --> 00:07:28,450
除了包含我们的整体的推理的引擎

255
00:07:28,450 --> 00:07:29,966
真正的后端的backend

256
00:07:29,966 --> 00:07:32,450
其实还包括一些推理的服务化的内容

257
00:07:32,450 --> 00:07:33,200
所以里面

258
00:07:33,200 --> 00:07:35,800
就有了MindIE service的相关的内容

259
00:07:35,800 --> 00:07:38,166
所以称它为一个框架不为过吧

260
00:07:38,166 --> 00:07:38,850
不为了证明

261
00:07:38,850 --> 00:07:40,566
我们看一下它有一些开发场景

262
00:07:40,566 --> 00:07:41,566
什么服务化部署

263
00:07:41,566 --> 00:07:42,683
大模型的推理

264
00:07:42,683 --> 00:07:44,600
文生图的推理

265
00:07:44,600 --> 00:07:46,116
其实也就是stable diffusion

266
00:07:46,116 --> 00:07:47,200
还有一些编译优化

267
00:07:47,200 --> 00:07:48,600
还有框架的推理

268
00:07:48,600 --> 00:07:49,366
那框架推理

269
00:07:49,366 --> 00:07:51,000
主要是对接两个不同的框架

270
00:07:51,000 --> 00:07:52,166
一个是MindSpore的框架

271
00:07:52,166 --> 00:07:53,400
一个是PyTorch框架

272
00:07:53,400 --> 00:07:55,050
可能对接PyTorch框架这个后端

273
00:07:55,050 --> 00:07:55,766
用的人

274
00:07:55,766 --> 00:07:58,316
更多包括现在的vLLM

275
00:07:58,316 --> 00:07:59,650
其实它后端

276
00:07:59,650 --> 00:08:01,450
除了跑自己的page attention以外

277
00:08:01,450 --> 00:08:03,800
它也对接PyTorch的后端的框架

278
00:08:03,800 --> 00:08:04,483
那基本上

279
00:08:04,483 --> 00:08:06,166
我们可以看到每一个厂商

280
00:08:06,166 --> 00:08:08,566
都会有自己的一个推理引擎

281
00:08:08,566 --> 00:08:09,800
针对大模型

282
00:08:10,316 --> 00:08:11,850
今天的视频非常水

283
00:08:11,850 --> 00:08:12,400
就图个乐

284
00:08:12,400 --> 00:08:14,600
我们现在来逐个的简单的分析一下

285
00:08:14,600 --> 00:08:16,483
可能每个也就那么一两页PPT

286
00:08:16,516 --> 00:08:17,450
就这么几句话

287
00:08:17,450 --> 00:08:19,683
首先我们来看一下Huggingface的TGI

288
00:08:19,683 --> 00:08:20,200
那ZOMI

289
00:08:20,200 --> 00:08:21,366
就截取了Huggingface

290
00:08:21,366 --> 00:08:23,116
上面的一些简单的信息

291
00:08:23,116 --> 00:08:24,050
那可以看到

292
00:08:24,050 --> 00:08:25,050
整个Huggingface里面

293
00:08:25,050 --> 00:08:27,166
其实整体做它的TGI

294
00:08:27,166 --> 00:08:28,483
贡献的人数并不多

295
00:08:28,483 --> 00:08:30,200
142个不过比较有意思

296
00:08:30,200 --> 00:08:31,566
我们看一下它的一个language

297
00:08:31,566 --> 00:08:34,250
基本上22%都是从rust去写

298
00:08:34,250 --> 00:08:36,683
有70%是做上层的封装

299
00:08:36,683 --> 00:08:38,800
那整体的后端的Backend

300
00:08:38,800 --> 00:08:40,800
其实大部分都是Rust来写

301
00:08:40,800 --> 00:08:42,883
那ZOMI觉得这里面还是蛮有意思

302
00:08:42,883 --> 00:08:43,683
就是TGI

303
00:08:43,683 --> 00:08:46,850
它虽然声称字里面用了page attention

304
00:08:46,850 --> 00:08:48,766
但是它的实际的吞吐性能一般

305
00:08:48,766 --> 00:08:49,800
然后预分配的时候

306
00:08:49,800 --> 00:08:52,283
显存也造成了蛮多的变化

307
00:08:52,283 --> 00:08:54,516
batch size其实增长之后

308
00:08:54,516 --> 00:08:56,883
你会发现就跑不动了这个大模型

309
00:08:57,000 --> 00:08:57,400
另外的话

310
00:08:57,400 --> 00:08:59,766
TGI的CPU跟GPU的调度主要是串型

311
00:08:59,766 --> 00:09:01,850
所以导致整体CPU调度的时候

312
00:09:01,850 --> 00:09:03,250
可能GPU是闲置

313
00:09:03,250 --> 00:09:04,683
然后吞吐性能比较差

314
00:09:04,683 --> 00:09:05,483
那TGI

315
00:09:05,483 --> 00:09:07,883
我觉得最大的问题就是两个

316
00:09:07,883 --> 00:09:09,166
第一个性能比较挫

317
00:09:09,200 --> 00:09:09,650
第二个

318
00:09:09,650 --> 00:09:11,316
他居然用Rust来去实现

319
00:09:11,316 --> 00:09:12,650
整体的那个调度的逻辑

320
00:09:12,650 --> 00:09:13,916
和底层细节的逻辑

321
00:09:13,916 --> 00:09:15,650
导致了很多像ZOMI这种

322
00:09:15,650 --> 00:09:17,800
可能自动C++跟Python的用户

323
00:09:17,800 --> 00:09:19,766
就没有办法很快速的上手

324
00:09:19,916 --> 00:09:20,566
那另外的话

325
00:09:20,566 --> 00:09:22,366
还有开发人员整体还是不够

326
00:09:22,366 --> 00:09:23,600
版本迭代比较慢

327
00:09:23,600 --> 00:09:26,050
真的不像Huggingface的一个出品

328
00:09:26,050 --> 00:09:28,650
我觉得确实做的可圈可点

329
00:09:30,166 --> 00:09:31,283
接着我们看一下

330
00:09:31,283 --> 00:09:34,400
第二个vLLM我现在业绩用的特别的多

331
00:09:34,400 --> 00:09:34,916
同样

332
00:09:34,916 --> 00:09:38,050
我们打开github的链接看一下相关的内容

333
00:09:38,050 --> 00:09:41,000
那整体vLLM的contribute是非常的多

334
00:09:41,000 --> 00:09:43,050
我们可以看到已经有700多个contribute

335
00:09:43,050 --> 00:09:44,083
还是非常的夸张

336
00:09:44,083 --> 00:09:45,850
现在也不断在增长

337
00:09:45,850 --> 00:09:47,766
另外的话language还是蛮有意思

338
00:09:47,766 --> 00:09:51,200
这里面大量的83%的都是用Python哦

339
00:09:51,200 --> 00:09:54,000
而CUDA你看跟C++还是非常的少

340
00:09:54,000 --> 00:09:55,400
CUDA也就占了11%

341
00:09:55,400 --> 00:09:56,716
可能这里面最核心

342
00:09:56,716 --> 00:09:59,400
就是用pageattention实现

343
00:09:59,400 --> 00:10:01,566
最核心的一些业务模块了

344
00:10:01,566 --> 00:10:03,166
那整体vLLM里面

345
00:10:03,166 --> 00:10:04,883
嗯一开始发源

346
00:10:04,883 --> 00:10:07,316
或者它的发展还是蛮有蛮蛮神奇

347
00:10:07,316 --> 00:10:09,516
从去年年终才开始

348
00:10:09,516 --> 00:10:11,716
就23年年终的时候才开始开源了

349
00:10:11,716 --> 00:10:14,450
原来作者只是为了实现page attention

350
00:10:14,450 --> 00:10:16,250
然后开源了一个简单

351
00:10:16,250 --> 00:10:17,850
page attention的实现的框架

352
00:10:18,050 --> 00:10:19,316
不过发展到今天为止

353
00:10:19,316 --> 00:10:20,800
整个vLLM的推理框架

354
00:10:20,800 --> 00:10:23,000
已经成为整个大语言模型推理框架

355
00:10:23,000 --> 00:10:24,883
一个非常重要的标杆了

356
00:10:24,883 --> 00:10:25,450
那这里面

357
00:10:25,450 --> 00:10:28,800
简单的分析它的一些优势跟劣势

358
00:10:28,800 --> 00:10:29,316
那优势

359
00:10:29,316 --> 00:10:32,200
就是它主要是由UCB去开发

360
00:10:32,566 --> 00:10:33,800
那什么是UCB

361
00:10:33,800 --> 00:10:34,483
就是UUUUQAQ

362
00:10:34,483 --> 00:10:37,000
 UUC Balcony呀

363
00:10:37,450 --> 00:10:39,966
俗称加州大学伯克利

364
00:10:39,966 --> 00:10:40,683
那这里面

365
00:10:40,683 --> 00:10:42,250
有着非常多的稳定的开发者

366
00:10:42,250 --> 00:10:42,966
而且作者

367
00:10:42,966 --> 00:10:45,450
主要是为了发明这个page attention

368
00:10:45,450 --> 00:10:46,000
来去写

369
00:10:46,000 --> 00:10:47,600
所以作者是一个在读的博士

370
00:10:47,600 --> 00:10:48,766
那比较多的时间

371
00:10:48,766 --> 00:10:50,650
去做一些大量的贡献

372
00:10:50,650 --> 00:10:52,850
整体来说整个vLLM的人员投入了比

373
00:10:52,850 --> 00:10:54,366
还是非常的好

374
00:10:54,366 --> 00:10:57,250
而且支持比较多的一些新的硬件

375
00:10:57,250 --> 00:10:58,483
因为它比较开放

376
00:10:58,483 --> 00:11:00,966
你可以发现很多人或者很多组织

377
00:11:00,966 --> 00:11:04,000
都可以成为vLLM的一个contributor

378
00:11:04,000 --> 00:11:04,850
或者commit

379
00:11:04,883 --> 00:11:05,283
那另外它

380
00:11:05,283 --> 00:11:07,683
它的社区的活跃度是非常的高

381
00:11:07,683 --> 00:11:09,600
里面有大量的issue和PR

382
00:11:09,650 --> 00:11:10,250
大量的poper

383
00:11:10,250 --> 00:11:12,200
都是以vLLM作为baseline来去开发

384
00:11:12,200 --> 00:11:13,000
另外它自己

385
00:11:13,000 --> 00:11:14,600
开发了一套比较完善

386
00:11:14,600 --> 00:11:16,366
成熟的一个benchmark

387
00:11:16,483 --> 00:11:16,916
另外的话

388
00:11:16,916 --> 00:11:18,083
它现在来说

389
00:11:18,083 --> 00:11:19,316
因为参与的人多了

390
00:11:19,316 --> 00:11:19,483
所以

391
00:11:19,483 --> 00:11:21,766
它拥有了各种各样的优化的手段了

392
00:11:21,766 --> 00:11:22,766
包括权重量化

393
00:11:22,766 --> 00:11:23,016
KV压缩

394
00:11:23,050 --> 00:11:24,166
specialty decode啦

395
00:11:24,166 --> 00:11:25,650
还有chunked prefill相关

396
00:11:25,650 --> 00:11:28,450
各种各样的特性也是慢慢的加进去了

397
00:11:28,566 --> 00:11:30,766
反正在现在的AI领域

398
00:11:30,766 --> 00:11:31,650
基本上你不开源

399
00:11:31,650 --> 00:11:33,000
你会发现用的人越少

400
00:11:33,000 --> 00:11:33,916
那用的人越多了

401
00:11:33,916 --> 00:11:35,250
他提供贡献的能力

402
00:11:35,250 --> 00:11:36,366
就越大整体的生态

403
00:11:36,366 --> 00:11:37,400
也就越强大

404
00:11:37,400 --> 00:11:39,200
那这个是AI跟传统的软件

405
00:11:39,200 --> 00:11:40,283
最大的区别

406
00:11:40,600 --> 00:11:41,050
另外的话

407
00:11:41,050 --> 00:11:42,800
我们看一下它的一些缺点

408
00:11:42,800 --> 00:11:44,883
说实话在V0.6.0之前

409
00:11:44,883 --> 00:11:46,800
vLLM的CPU跟GPU其实是

410
00:11:46,966 --> 00:11:48,083
串行调度

411
00:11:48,316 --> 00:11:49,850
导致就跟TGI一样了

412
00:11:49,850 --> 00:11:51,083
所以CPU的计算的时候

413
00:11:51,083 --> 00:11:53,000
GPU闲置吞吐相对比较差

414
00:11:53,000 --> 00:11:55,050
不过现在已经正在重构整改了

415
00:11:55,050 --> 00:11:58,250
现在 0.6.2版本已经推出出来了

416
00:11:58,250 --> 00:12:00,566
基本上这个问题了不是大问题

417
00:12:00,850 --> 00:12:03,083
另外的话就是合入的功能越来越多

418
00:12:03,083 --> 00:12:05,000
所以说现在在高负载情况下

419
00:12:05,000 --> 00:12:06,366
整个MPU的利用率

420
00:12:06,366 --> 00:12:08,650
会降到比较低的情况下

421
00:12:08,650 --> 00:12:09,483
所以说

422
00:12:09,483 --> 00:12:10,366
贡献的越多

423
00:12:10,366 --> 00:12:11,916
还是得要收一下

424
00:12:11,916 --> 00:12:12,766
刹一下车

425
00:12:12,766 --> 00:12:13,850
可能部分时间

426
00:12:13,850 --> 00:12:15,283
还得做一些重构

427
00:12:15,283 --> 00:12:16,200
那所谓的重构

428
00:12:16,200 --> 00:12:17,483
就是现在的代码来说

429
00:12:17,483 --> 00:12:19,483
整个vLLM相对的臃肿

430
00:12:19,800 --> 00:12:20,966
二次的开发和定制

431
00:12:20,966 --> 00:12:23,200
变得有一点困难了

432
00:12:23,200 --> 00:12:24,850
不过不妨碍他在社区里面

433
00:12:24,850 --> 00:12:27,116
还是站到一哥的角色

434
00:12:28,316 --> 00:12:29,083
第三个我们看一下

435
00:12:29,083 --> 00:12:32,800
整个sglang相关的一些特性

436
00:12:32,800 --> 00:12:34,250
那sglang蛮有意思

437
00:12:34,250 --> 00:12:35,200
里面的contributor

438
00:12:35,200 --> 00:12:36,316
其实跟TGI差不多

439
00:12:36,316 --> 00:12:37,516
也是100多号人

440
00:12:37,516 --> 00:12:38,450
但是language

441
00:12:38,450 --> 00:12:39,650
你看到百分之

442
00:12:39,650 --> 00:12:42,450
基本上98都是用Python来去实现

443
00:12:42,450 --> 00:12:43,650
这里面还蛮有意思

444
00:12:43,650 --> 00:12:44,450
整个sglang

445
00:12:44,450 --> 00:12:46,250
其实也是u u u UBC

446
00:12:46,250 --> 00:12:48,250
也就是加州伯克利分校

447
00:12:48,250 --> 00:12:49,683
里面的博士生去

448
00:12:49,683 --> 00:12:50,916
提供的那这里面

449
00:12:50,916 --> 00:12:53,516
其实借鉴了lightllM的一个框架的趋势

450
00:12:53,516 --> 00:12:55,166
它里面整体的吞吐的性能

451
00:12:55,166 --> 00:12:56,366
是做的非常好

452
00:12:56,366 --> 00:12:58,650
而且用了多进程的GMP的传输

453
00:12:58,650 --> 00:13:01,366
中间来去代替掉CPU的开销

454
00:13:01,366 --> 00:13:01,850
所以导致

455
00:13:01,850 --> 00:13:03,450
我们在高负载的情况下

456
00:13:03,450 --> 00:13:04,716
已经Batch Size得非常大哟

457
00:13:04,716 --> 00:13:07,000
嗯很多客户访问请求的时候

458
00:13:07,000 --> 00:13:08,716
整体GPU的利用率和吞吐

459
00:13:08,716 --> 00:13:09,716
做的比较好

460
00:13:09,716 --> 00:13:11,366
所以整个SGlang它的性能

461
00:13:11,366 --> 00:13:13,716
还是相对来说非常的优

462
00:13:13,716 --> 00:13:14,283
另外的话

463
00:13:14,283 --> 00:13:15,800
整个GPU的代码的口拓展性

464
00:13:15,800 --> 00:13:16,566
还是非常好

465
00:13:16,566 --> 00:13:18,166
然后代码结构比较清晰

466
00:13:18,166 --> 00:13:19,450
大家也可以去看一下

467
00:13:19,450 --> 00:13:21,250
SGlang大量都是Python的代码

468
00:13:21,250 --> 00:13:22,566
所以非常的好看

469
00:13:22,566 --> 00:13:23,116
另外的话

470
00:13:23,116 --> 00:13:24,366
SGlang的开发维护者

471
00:13:24,366 --> 00:13:25,800
因为都是一些学生嘛

472
00:13:25,800 --> 00:13:28,200
他非常积极的去回复一些issue

473
00:13:28,200 --> 00:13:30,050
他没有企业非常严重的KPI

474
00:13:30,050 --> 00:13:32,683
导致就你你定了KPI让人家回复issue

475
00:13:32,766 --> 00:13:35,316
就导致很多研发人员就不想去回复了

476
00:13:35,316 --> 00:13:37,116
或者觉得他是一种工作的负担

477
00:13:37,116 --> 00:13:39,283
我觉得这个事情都是个双刃剑

478
00:13:39,283 --> 00:13:42,083
所以为什么很多大公司有大公司病了

479
00:13:42,283 --> 00:13:43,850
我们还是回到这里面了

480
00:13:43,850 --> 00:13:45,366
不吐槽太多

481
00:13:45,766 --> 00:13:46,283
那这里面

482
00:13:46,283 --> 00:13:49,166
我们看一下整体的下个内容就是

483
00:13:49,166 --> 00:13:51,716
LMDeploy还蛮神奇

484
00:13:51,716 --> 00:13:53,250
是由上智院去开发

485
00:13:53,250 --> 00:13:54,400
虽然这里面的贡献值

486
00:13:54,400 --> 00:13:55,800
相对来说是最低

487
00:13:55,800 --> 00:13:56,316
但是

488
00:13:56,316 --> 00:13:58,166
LMDeploy跟刚才的有个最大的区别

489
00:13:58,166 --> 00:14:00,516
它的Python的占比只有50%哦

490
00:14:00,766 --> 00:14:03,083
大量的都是C++的代码

491
00:14:03,083 --> 00:14:04,566
那这边还蛮神奇

492
00:14:04,566 --> 00:14:06,200
在整个推理框架的后端

493
00:14:06,200 --> 00:14:07,966
推理引擎里面

494
00:14:08,050 --> 00:14:09,800
C++占比特别的多

495
00:14:09,800 --> 00:14:10,850
那跟传统的一样

496
00:14:10,850 --> 00:14:12,000
可能传统的推理引擎了

497
00:14:12,000 --> 00:14:14,683
我们大量的都是用C++来去实现

498
00:14:14,683 --> 00:14:16,716
那我们看到刚才的TGI也好

499
00:14:16,716 --> 00:14:18,883
还有 vLLM 还有SGlang

500
00:14:18,883 --> 00:14:20,600
大部分都是用Python来实现

501
00:14:20,600 --> 00:14:20,966
这里面

502
00:14:20,966 --> 00:14:22,250
看一下有什么区别

503
00:14:22,283 --> 00:14:22,916
那这里面

504
00:14:22,916 --> 00:14:23,683
主要是哦

505
00:14:23,683 --> 00:14:24,683
不是商汤

506
00:14:24,683 --> 00:14:24,916
商汤

507
00:14:24,916 --> 00:14:27,366
跟上海人工智能实验室团队一起开发

508
00:14:27,366 --> 00:14:28,800
所以这么ZOMI记错了

509
00:14:28,800 --> 00:14:31,166
相比起vLLM跟SGlang

510
00:14:31,166 --> 00:14:32,166
它的python实现

511
00:14:32,166 --> 00:14:33,850
lmdeploy的调度和runtime

512
00:14:33,883 --> 00:14:35,050
就底层的Backend

513
00:14:35,050 --> 00:14:36,916
主要是使用了C++去实现

514
00:14:36,916 --> 00:14:39,050
然后CPU的调度数量是比较优

515
00:14:39,050 --> 00:14:40,400
高负载情况下

516
00:14:40,400 --> 00:14:41,966
GPU利用率是最高

517
00:14:41,966 --> 00:14:43,200
因为是c加实现

518
00:14:43,200 --> 00:14:44,116
独一无二

519
00:14:44,450 --> 00:14:44,850
这里面

520
00:14:44,850 --> 00:14:46,800
对于多模态的支持也是非常好

521
00:14:46,800 --> 00:14:48,316
而且它内置了很多

522
00:14:48,316 --> 00:14:49,716
多模态的大模型

523
00:14:49,716 --> 00:14:51,166
基本上拿来即用

524
00:14:51,166 --> 00:14:53,316
所以你不用关心C++的比例高了

525
00:14:53,316 --> 00:14:54,166
是不是不好用

526
00:14:54,166 --> 00:14:55,483
倒是没有这个情况

527
00:14:55,483 --> 00:14:56,000
另外的话

528
00:14:56,000 --> 00:14:58,400
对国内的GPU的适配厂商来说

529
00:14:58,400 --> 00:14:59,316
还是比较好

530
00:14:59,316 --> 00:15:00,916
因为上海人工研究院

531
00:15:00,916 --> 00:15:02,566
或者上海人工研究实验室

532
00:15:02,566 --> 00:15:04,316
它其实背负了一个比较大的使命

533
00:15:04,316 --> 00:15:06,166
就是我不能买英伟达

534
00:15:06,166 --> 00:15:07,966
我只能用国产的芯片

535
00:15:07,966 --> 00:15:09,316
所以它只能去适配各种各样

536
00:15:09,316 --> 00:15:11,450
国产的一些厂商的硬件

537
00:15:11,450 --> 00:15:13,200
这是它的一个主要的优势了

538
00:15:13,483 --> 00:15:14,166
另外的话

539
00:15:14,166 --> 00:15:15,566
其实缺点也比较明显

540
00:15:15,566 --> 00:15:16,883
就是它的开发语言太少了

541
00:15:16,883 --> 00:15:18,683
功能相对起vLLM跟SGlang

542
00:15:18,683 --> 00:15:20,250
还是少了很多

543
00:15:20,250 --> 00:15:22,083
只是它的模型比较丰富

544
00:15:22,116 --> 00:15:22,766
另外的话

545
00:15:22,766 --> 00:15:24,366
从Python现在来看

546
00:15:24,366 --> 00:15:26,766
实现的LLM大语言模型其实已经够用了

547
00:15:26,766 --> 00:15:29,800
那你把C++跟rust的这种引进去

548
00:15:29,800 --> 00:15:31,366
到底是利大于弊

549
00:15:31,366 --> 00:15:32,850
还是弊大于利

550
00:15:32,850 --> 00:15:34,916
我觉得大家可以讨论一下哦

551
00:15:35,316 --> 00:15:37,566
为什么现在有些推理框架

552
00:15:37,566 --> 00:15:40,083
用纯Python的性能做的也挺好

553
00:15:40,083 --> 00:15:42,516
为什么有些用C++性能也做的挺好

554
00:15:42,516 --> 00:15:44,083
但是用的也比较少

555
00:15:44,250 --> 00:15:45,283
到底是哪种更优了

556
00:15:45,283 --> 00:15:46,516
我觉得大家可以探讨

557
00:15:46,516 --> 00:15:47,166
另外的话

558
00:15:47,166 --> 00:15:48,766
现在AI场景python的开发效率

559
00:15:48,766 --> 00:15:51,200
确实比其他的语言优势大太多了

560
00:15:51,200 --> 00:15:52,966
为什么大家可能用LMdeploy

561
00:15:52,966 --> 00:15:53,800
用的不大顺

562
00:15:53,800 --> 00:15:54,883
因为说实话

563
00:15:54,883 --> 00:15:57,050
它的一些特性还是相对比较少

564
00:15:57,050 --> 00:15:58,683
而且开发特性的难度

565
00:15:58,683 --> 00:15:59,766
还是有

566
00:16:00,516 --> 00:16:02,883
一口气没有NG

567
00:16:03,166 --> 00:16:04,116
讲完刚才的内容之后

568
00:16:04,116 --> 00:16:06,250
我们现在来看一下整个性能类比

569
00:16:06,250 --> 00:16:07,250
当然上面的内容

570
00:16:07,250 --> 00:16:08,283
其实是随着时间

571
00:16:08,283 --> 00:16:09,483
不断的去更新

572
00:16:09,483 --> 00:16:10,650
不断的去发展

573
00:16:10,650 --> 00:16:11,716
所以中间过程当中

574
00:16:11,716 --> 00:16:13,050
可能有一些看护了

575
00:16:13,050 --> 00:16:14,250
或者你已经看的时候

576
00:16:14,250 --> 00:16:15,683
这个已经过期了

577
00:16:15,683 --> 00:16:17,400
所以欢迎大家来指正和吐槽

578
00:16:17,716 --> 00:16:20,200
那接着我们看一下相关的一些性能了

579
00:16:20,200 --> 00:16:20,600
这里面

580
00:16:20,600 --> 00:16:23,250
其实相关的一些网络上的文章

581
00:16:23,250 --> 00:16:23,883
我们在最后

582
00:16:23,883 --> 00:16:25,200
其实有相关的链接

583
00:16:25,283 --> 00:16:27,000
里面就做了一些简单的性能分析

584
00:16:27,000 --> 00:16:27,800
包括lmdeploy呀

585
00:16:27,800 --> 00:16:28,716
还有TGI呀

586
00:16:28,716 --> 00:16:30,316
还有那个vLLM相关

587
00:16:30,316 --> 00:16:31,166
那整体

588
00:16:31,166 --> 00:16:33,650
Llama 8B的这个网络模型来看

589
00:16:33,650 --> 00:16:34,083
这里面

590
00:16:34,083 --> 00:16:35,566
后面我们会重点去讲讲

591
00:16:35,566 --> 00:16:38,083
大模型的一些相关的性能指标

592
00:16:38,083 --> 00:16:38,566
那这里面

593
00:16:38,566 --> 00:16:41,000
我们简单了解一下TTFT就好了

594
00:16:41,000 --> 00:16:41,683
那整体来说

595
00:16:41,683 --> 00:16:42,650
我们会看到

596
00:16:42,650 --> 00:16:45,366
它主要是看有多少个用户请求

597
00:16:45,366 --> 00:16:46,916
所以我们的利用率

598
00:16:46,916 --> 00:16:48,116
或者我们的吞吐

599
00:16:48,116 --> 00:16:49,733
就变得非常的重要

600
00:16:49,733 --> 00:16:51,766
了那整个用户请求里面

601
00:16:51,766 --> 00:16:52,966
我们就可以看到

602
00:16:52,966 --> 00:16:53,716
在基本上

603
00:16:53,716 --> 00:16:55,800
TGI的Backend还是相对比较好

604
00:16:55,800 --> 00:16:56,650
在Batch Size=50的时候

605
00:16:56,966 --> 00:16:59,516
英伟达的tensor RT还是性能比较好

606
00:16:59,516 --> 00:17:01,450
这也是原厂自己设置

607
00:17:01,450 --> 00:17:02,283
但是vLLM

608
00:17:02,283 --> 00:17:03,450
随着我们的用户的请求

609
00:17:03,450 --> 00:17:04,883
访问量变大的时候

610
00:17:04,883 --> 00:17:06,800
它整体的性能是变低

611
00:17:06,800 --> 00:17:09,250
所以基本上我们如果从性能来看

612
00:17:09,250 --> 00:17:12,000
说实话vLLM不是个最好的选择

613
00:17:12,283 --> 00:17:12,716
另外的话

614
00:17:12,716 --> 00:17:14,050
Huggingface出品的TGI

615
00:17:14,050 --> 00:17:15,516
说实话效果还是不错

616
00:17:15,516 --> 00:17:15,966
但是

617
00:17:15,966 --> 00:17:18,316
上海人工研究院里面的一个lmdeploy

618
00:17:18,316 --> 00:17:19,650
大概在各种情况下

619
00:17:19,650 --> 00:17:21,600
其实非常的中规中矩

620
00:17:21,600 --> 00:17:23,116
没有自己独立的特色

621
00:17:23,116 --> 00:17:23,683
而且

622
00:17:23,683 --> 00:17:25,883
用起来还是因为使用了C++大量

623
00:17:25,883 --> 00:17:27,366
所以导致它不太好用了

624
00:17:27,366 --> 00:17:27,916
所以ZOMI

625
00:17:27,916 --> 00:17:30,600
其实个人不是很看好lmdeploy

626
00:17:30,600 --> 00:17:31,200
不过

627
00:17:31,200 --> 00:17:33,483
你说直接它已经内置了很多模型

628
00:17:33,483 --> 00:17:34,850
那这个还是可以直接用

629
00:17:35,366 --> 00:17:36,400
另外的话我们看一下

630
00:17:36,400 --> 00:17:38,316
第二个性能的评价分析

631
00:17:38,316 --> 00:17:41,450
就同样的用了Llama 38B 的一个模型

632
00:17:41,450 --> 00:17:43,366
TOKEN Generation Rate of Different Backends

633
00:17:43,516 --> 00:17:45,283
Different Backends

634
00:17:45,483 --> 00:17:47,766
那这边也是通过不同的后端

635
00:17:47,766 --> 00:17:51,200
去看一下TOKEN的一个生成的一个速率

636
00:17:51,200 --> 00:17:53,450
那这边可以看到基本上都你觉得嗯

637
00:17:53,450 --> 00:17:55,366
绿色的就lmdeploy呀

638
00:17:55,366 --> 00:17:56,450
在decode阶段

639
00:17:56,450 --> 00:17:57,800
确实因为C++去写了

640
00:17:57,800 --> 00:17:59,516
所以说它后面的性能

641
00:17:59,516 --> 00:18:01,966
整体来说是相对来说比较好

642
00:18:02,450 --> 00:18:05,200
另外我们可以看一下关于Q4证明

643
00:18:05,200 --> 00:18:06,483
Q4不是第四季度

644
00:18:06,483 --> 00:18:08,166
而是Quant的4

645
00:18:08,166 --> 00:18:09,166
 4比之量快就低

646
00:18:09,166 --> 00:18:10,316
比量快的时候

647
00:18:10,316 --> 00:18:12,083
不同的后段的一个情况

648
00:18:12,083 --> 00:18:13,600
基本上跟第一个差不多

649
00:18:13,600 --> 00:18:14,850
TG还是相对比较好

650
00:18:14,966 --> 00:18:15,283
然后

651
00:18:15,283 --> 00:18:17,000
可能当我们的用户规模越大了

652
00:18:17,000 --> 00:18:18,516
使用原厂出质的比较好

653
00:18:18,516 --> 00:18:19,250
那vLLM

654
00:18:19,250 --> 00:18:20,800
更多的是学术界

655
00:18:20,800 --> 00:18:21,516
同样的证明

656
00:18:21,516 --> 00:18:23,716
也做了一个简单的对比

657
00:18:23,716 --> 00:18:24,516
l m Depoint

658
00:18:24,516 --> 00:18:25,566
因为用C++写

659
00:18:25,566 --> 00:18:28,366
所以它整体的一个单token或者单decode

660
00:18:28,366 --> 00:18:30,283
就回归的一个时候的阶段的性能

661
00:18:30,283 --> 00:18:31,000
来比较好

662
00:18:31,000 --> 00:18:33,683
另外的话就是原厂商的提供

663
00:18:33,683 --> 00:18:34,850
原厂商确实

664
00:18:35,083 --> 00:18:36,566
原厂提供相对来说比较好

665
00:18:36,566 --> 00:18:38,850
但是原厂提供不一定好用

666
00:18:38,850 --> 00:18:41,316
这也是大家做博弈的过程当中

667
00:18:42,166 --> 00:18:43,566
那相对比较水

668
00:18:43,566 --> 00:18:44,600
ZOM比较轻松

669
00:18:44,600 --> 00:18:47,083
我们先来一个简单的总结和思考了

670
00:18:47,083 --> 00:18:50,000
这里面就整个大模型的一个推理框架

671
00:18:50,000 --> 00:18:53,400
相关的一个横向的对比了

672
00:18:53,400 --> 00:18:53,966
LMDeploy

673
00:18:53,966 --> 00:18:54,600
tensorRT

674
00:18:54,600 --> 00:18:56,200
还有vLLM还有MLC-LLM

675
00:18:56,200 --> 00:18:56,916
还有TGI

676
00:18:56,916 --> 00:18:58,200
这里面没放MindIE 

677
00:18:58,200 --> 00:19:00,083
是因为确实它不太好比

678
00:19:00,083 --> 00:19:02,250
MindIE相对对外的资料来说

679
00:19:02,250 --> 00:19:03,800
现在来说还是相对比较少

680
00:19:03,800 --> 00:19:05,283
希望那个华为昇腾

681
00:19:05,283 --> 00:19:07,050
能够提供更多的对外资料

682
00:19:07,050 --> 00:19:09,566
同时哦同时因为这个事情

683
00:19:09,566 --> 00:19:10,600
是我下班说

684
00:19:10,600 --> 00:19:11,450
我上班的时候

685
00:19:11,450 --> 00:19:12,766
还是各种各样

686
00:19:12,766 --> 00:19:15,050
给MindIE补充各种各样的资料

687
00:19:15,050 --> 00:19:15,916
还有能力

688
00:19:15,916 --> 00:19:16,766
那不管

689
00:19:16,766 --> 00:19:18,483
我们先来看一下简单的思考

690
00:19:18,483 --> 00:19:19,483
这么多推理引擎

691
00:19:19,483 --> 00:19:20,516
或者这么多推理框架

692
00:19:20,516 --> 00:19:21,516
我们应该怎么选

693
00:19:22,000 --> 00:19:23,083
首先这里面

694
00:19:23,083 --> 00:19:24,766
ZOMI觉得还是蛮有意思

695
00:19:24,766 --> 00:19:26,850
如果我们不具备二次开发

696
00:19:26,850 --> 00:19:27,650
想要快速的部署

697
00:19:27,650 --> 00:19:28,366
开箱即用

698
00:19:28,366 --> 00:19:30,516
说实话vLLM是最好的一个选择

699
00:19:30,516 --> 00:19:31,600
因为它比较简单

700
00:19:31,600 --> 00:19:32,083
比较易用

701
00:19:32,083 --> 00:19:33,916
 用性还是非常的强

702
00:19:34,250 --> 00:19:35,050
另外的话

703
00:19:35,116 --> 00:19:37,316
如果我们需要对接大量

704
00:19:37,316 --> 00:19:38,916
国产的硬件厂商

705
00:19:38,916 --> 00:19:42,366
其实我们其实更好的选择是lmdeploy

706
00:19:42,366 --> 00:19:44,050
因为它已经内置了很多

707
00:19:44,050 --> 00:19:44,966
多模态的大模型

708
00:19:44,966 --> 00:19:46,916
还有那个大语言模型也好

709
00:19:46,916 --> 00:19:49,000
还有内置的支持很多不同的GPU

710
00:19:49,166 --> 00:19:49,883
那另外的话

711
00:19:49,883 --> 00:19:52,283
如果对性能有极致的追求的话

712
00:19:52,450 --> 00:19:52,883
说实话

713
00:19:52,883 --> 00:19:56,050
选择SGlang的是个很好的一个方案

714
00:19:56,283 --> 00:19:58,966
如果你针对某个昇腾的厂商

715
00:19:58,966 --> 00:20:00,716
或者针对昇腾的GPU

716
00:20:00,716 --> 00:20:01,200
去选择

717
00:20:01,200 --> 00:20:03,450
其实MindIE是一个最好的选择

718
00:20:03,450 --> 00:20:04,450
因为原厂出厂

719
00:20:04,450 --> 00:20:05,850
会经过大量的优化

720
00:20:05,850 --> 00:20:08,083
包括如果你真正的选择大规模用户

721
00:20:08,083 --> 00:20:09,083
并行的时候

722
00:20:09,083 --> 00:20:11,600
或大规模用户访问请求的时候

723
00:20:11,600 --> 00:20:12,966
Tensor RT LM

724
00:20:12,966 --> 00:20:15,283
英伟达自己出厂的也是性能最好

725
00:20:15,450 --> 00:20:16,050
不过

726
00:20:16,050 --> 00:20:18,083
如果你是一个想中立的态度

727
00:20:18,083 --> 00:20:20,000
你去SGlang来进行魔改了

728
00:20:20,000 --> 00:20:21,166
ZOMI觉得也是OK

729
00:20:21,166 --> 00:20:21,966
那今天内容

730
00:20:21,966 --> 00:20:22,966
已经到此为止了

731
00:20:22,966 --> 00:20:24,366
我们简单的看了一下

732
00:20:24,366 --> 00:20:25,766
整个业界的推理框架

733
00:20:25,766 --> 00:20:26,650
到底有哪些内容

734
00:20:26,650 --> 00:20:28,116
然后我觉得ZOMI更重要

735
00:20:28,116 --> 00:20:29,683
跟大家一起去思考一下

736
00:20:29,683 --> 00:20:30,116
为什么

737
00:20:30,116 --> 00:20:33,000
我们对它叫做大模型的推理框架

738
00:20:33,000 --> 00:20:33,450
另外的话

739
00:20:33,450 --> 00:20:34,316
大模型推理框架

740
00:20:34,316 --> 00:20:35,116
很重要的目标

741
00:20:35,116 --> 00:20:37,566
就是实现高吞吐和低延迟

742
00:20:37,566 --> 00:20:38,483
所以不管哪个框架

743
00:20:38,483 --> 00:20:40,966
我们其实最终看的就是这两个指标

744
00:20:41,083 --> 00:20:44,166
尽可能的实现高吞吐和低延迟

745
00:20:44,200 --> 00:20:44,883
今天内容

746
00:20:44,883 --> 00:20:45,566
先到这里为止

747
00:20:45,566 --> 00:20:46,050
谢谢各位

748
00:20:46,050 --> 00:20:46,916
拜了个拜

