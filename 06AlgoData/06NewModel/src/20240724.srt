1
00:00:00,000 --> 00:00:01,916
内容/录制:Z0MI酱，视频剪辑/字幕:梁嘉铭

2
00:00:05,600 --> 00:00:05,883
哈喽

3
00:00:05,883 --> 00:00:07,166
大家晚上好

4
00:00:07,400 --> 00:00:08,766
今天我们来重点分析一下

5
00:00:08,766 --> 00:00:11,850
Llama3.1一个具体技术的洞察

6
00:00:11,850 --> 00:00:13,166
最近Llama3.1

7
00:00:13,166 --> 00:00:14,166
确实非常的火

8
00:00:14,166 --> 00:00:16,083
我们在今天的这个视频里面

9
00:00:16,083 --> 00:00:19,000
主要是跟大家去分享三个内容

10
00:00:19,000 --> 00:00:21,400
第一个就是Llama3.1的开源

11
00:00:21,600 --> 00:00:23,250
真正的参数量最大

12
00:00:23,250 --> 00:00:25,050
还有性能最好的开源模型

13
00:00:25,050 --> 00:00:27,450
我们来看一下它有什么具体的好处

14
00:00:27,450 --> 00:00:28,600
和它一个性能的效果

15
00:00:28,600 --> 00:00:29,600
到底是怎么样

16
00:00:30,200 --> 00:00:31,566
接着在第二个内容

17
00:00:31,566 --> 00:00:33,000
我们重点去观察一下

18
00:00:33,000 --> 00:00:35,850
Llama3.1 一个技术分析

19
00:00:35,883 --> 00:00:36,966
那这里面的技术分析

20
00:00:36,966 --> 00:00:39,400
我们就会从数据模型结构

21
00:00:39,450 --> 00:00:41,883
预训练和后处理四个部分

22
00:00:41,883 --> 00:00:44,050
跟大家一起去简单的分享

23
00:00:44,050 --> 00:00:44,933
那在最后

24
00:00:44,933 --> 00:00:46,200
我们还是要回顾一下

25
00:00:46,200 --> 00:00:47,733
对业界的一个具体的影响

26
00:00:47,733 --> 00:00:48,800
和关于业界的猜想

27
00:00:48,800 --> 00:00:50,933
特别是对百模厂商的一些冲击

28
00:00:50,933 --> 00:00:52,200
还有我们对产业的思考

29
00:00:52,200 --> 00:00:55,333
包括对大模型的一些具体的思考点

30
00:00:56,050 --> 00:00:57,800
在进入正式的内容之前

31
00:00:57,800 --> 00:00:58,400
我们看一下

32
00:00:58,400 --> 00:00:59,883
在整个Llama系列

33
00:00:59,883 --> 00:01:01,000
或者整个大模型里面

34
00:01:01,000 --> 00:01:01,533
我们看到

35
00:01:01,533 --> 00:01:05,000
现在的模型参数量越来越大

36
00:01:05,000 --> 00:01:05,850
而越往上

37
00:01:05,850 --> 00:01:08,283
就模型的效果确实也越来越好

38
00:01:08,283 --> 00:01:08,966
也就证明着

39
00:01:08,966 --> 00:01:10,883
我们的scaling law还没有失效

40
00:01:11,000 --> 00:01:14,283
scaling law也在不断的在增长

41
00:01:14,566 --> 00:01:17,283
这个大家一直在遵循相关的规则

42
00:01:17,366 --> 00:01:19,200
在真正打开Llama3.1之前

43
00:01:19,200 --> 00:01:20,366
我们主要看一下

44
00:01:20,366 --> 00:01:21,533
下面的三条链接

45
00:01:21,533 --> 00:01:22,683
就模型使用的API

46
00:01:22,683 --> 00:01:24,883
第一个Llama3.1一个具体文章

47
00:01:24,883 --> 00:01:25,083
还有

48
00:01:25,083 --> 00:01:27,933
Llama3.1 官网具体的内容

49
00:01:27,933 --> 00:01:29,333
那现在我们打开官网

50
00:01:29,733 --> 00:01:31,683
来看看里面有什么出彩的点

51
00:01:32,766 --> 00:01:33,400
现在我们

52
00:01:33,400 --> 00:01:35,650
简单跟大家一起去过一下

53
00:01:35,650 --> 00:01:37,566
Llama3.1 官网

54
00:01:37,566 --> 00:01:39,533
我们可以看到整个Llama3.1里面

55
00:01:39,533 --> 00:01:42,250
一共分开三个类型的大模型

56
00:01:42,250 --> 00:01:43,283
或者三个参数的大模型

57
00:01:43,283 --> 00:01:43,766
一个是8B

58
00:01:43,766 --> 00:01:44,450
一个是70B

59
00:01:44,450 --> 00:01:46,400
一个是405B

60
00:01:46,400 --> 00:01:47,883
也就是4,500亿

61
00:01:48,050 --> 00:01:48,650
三个大模型

62
00:01:48,650 --> 00:01:50,133
都可以同期的下载

63
00:01:50,133 --> 00:01:51,600
来看一下这个大模型

64
00:01:51,600 --> 00:01:52,566
或者Llama这个大模型

65
00:01:52,566 --> 00:01:54,400
这一次跟之前的有什么区别

66
00:01:54,450 --> 00:01:56,133
那最大的我们可以看到这个图

67
00:01:56,133 --> 00:01:58,400
它引入了一个多模态的场景

68
00:01:58,400 --> 00:01:59,933
我们可以输入其中的prompt

69
00:01:59,933 --> 00:02:01,050
然后通过我们的工具

70
00:02:01,050 --> 00:02:04,600
去使用里面相关的一些内容和组件

71
00:02:04,600 --> 00:02:05,800
那我们输进去一个对话

72
00:02:05,800 --> 00:02:07,250
他可以弹出一些一堆工具

73
00:02:07,250 --> 00:02:09,766
帮我对我们的图片进行解析

74
00:02:10,050 --> 00:02:10,933
那接着我们看一下

75
00:02:10,933 --> 00:02:12,483
这里面的这一个图了

76
00:02:12,483 --> 00:02:13,533
第二个内容

77
00:02:13,533 --> 00:02:15,133
同样也可以做一些图文的翻译

78
00:02:15,133 --> 00:02:16,850
和故事的生成

79
00:02:16,850 --> 00:02:18,483
那右边也是我们可以看到

80
00:02:18,483 --> 00:02:20,283
在整个Llama3.1里面

81
00:02:20,283 --> 00:02:23,200
已经引入了一种多模态的大模型

82
00:02:23,200 --> 00:02:26,333
可以跟图说话和灯图进行一个互动

83
00:02:26,566 --> 00:02:27,400
当然除了文字以外

84
00:02:27,400 --> 00:02:29,883
我们还可以语音的相关的内容的生成

85
00:02:29,883 --> 00:02:31,650
还有代码相关的生成的内容

86
00:02:31,650 --> 00:02:32,800
非常的多

87
00:02:32,800 --> 00:02:33,933
Make Llama your own

88
00:02:33,933 --> 00:02:36,133
也就我们可以在本地的进行一个部署

89
00:02:36,133 --> 00:02:36,766
而现在

90
00:02:36,766 --> 00:02:38,933
他已经在很多不同的平台上面

91
00:02:38,933 --> 00:02:40,533
做了一个很好的部署

92
00:02:40,533 --> 00:02:42,600
而且部署的一个效果

93
00:02:42,600 --> 00:02:45,400
也是说明他自己非常的牛逼

94
00:02:45,400 --> 00:02:46,933
那可以看到这里面的Benchmark

95
00:02:46,933 --> 00:02:47,566
确实

96
00:02:47,566 --> 00:02:50,600
整体它以GPT-4来进行对比

97
00:02:50,600 --> 00:02:53,083
还有的claude sonnet整个模型进行对比

98
00:02:53,083 --> 00:02:55,133
它的整体效果非常的好

99
00:02:55,133 --> 00:02:56,450
那效果我们就不用怀疑

100
00:02:56,450 --> 00:02:57,933
那模型的价格

101
00:02:57,933 --> 00:02:59,200
在不同的平台里面

102
00:02:59,200 --> 00:03:00,800
也是非常的优惠的

103
00:03:01,133 --> 00:03:02,850
了解完Llama3.1的基本信息之后

104
00:03:02,850 --> 00:03:03,566
我们现在

105
00:03:03,566 --> 00:03:05,933
打开Llama3.1的一个具体的官网

106
00:03:05,933 --> 00:03:07,366
里面就说了这篇文章

107
00:03:07,366 --> 00:03:09,083
其实他有很多的技术的细节

108
00:03:09,083 --> 00:03:10,083
都在这篇paper里面

109
00:03:10,083 --> 00:03:11,333
我们点击这篇paper

110
00:03:11,333 --> 00:03:12,400
然后进行下载

111
00:03:13,050 --> 00:03:14,883
现在我们简单的解读一下

112
00:03:14,883 --> 00:03:17,766
Llama 3这个head of model是这个网络模型

113
00:03:17,850 --> 00:03:19,200
里面这篇技术文章

114
00:03:19,200 --> 00:03:20,400
讲了一些什么内容

115
00:03:20,400 --> 00:03:21,333
我们首先

116
00:03:21,333 --> 00:03:24,083
肯定来看看左边的大纲

117
00:03:24,083 --> 00:03:25,200
有一个预训练

118
00:03:25,200 --> 00:03:27,333
还有一个训练后post training

119
00:03:27,333 --> 00:03:28,683
还有它的结果inference

120
00:03:28,683 --> 00:03:29,800
还有vision experience

121
00:03:30,000 --> 00:03:30,850
还speed experience

122
00:03:31,200 --> 00:03:32,483
也就是后面这两个模块

123
00:03:32,483 --> 00:03:34,050
重点介绍它的大模型

124
00:03:34,050 --> 00:03:35,366
或者它的一个多模态

125
00:03:35,366 --> 00:03:36,733
到底是怎么去实现

126
00:03:36,733 --> 00:03:37,400
那这里面

127
00:03:37,400 --> 00:03:38,883
其实我们有很多的内容

128
00:03:38,883 --> 00:03:39,366
就讲了

129
00:03:39,366 --> 00:03:41,933
它除了提供最基本的模型以外

130
00:03:41,933 --> 00:03:43,366
它会提供一个Instruct

131
00:03:43,366 --> 00:03:44,733
也就是经过微调

132
00:03:44,733 --> 00:03:46,400
或者一个post training之后

133
00:03:46,400 --> 00:03:47,683
一个具体的网络模型

134
00:03:47,683 --> 00:03:50,250
也就是它一共虽然是三个参数量

135
00:03:50,250 --> 00:03:51,366
但三个参数量

136
00:03:51,366 --> 00:03:53,850
它配套的一共有每一个参数量

137
00:03:53,850 --> 00:03:55,283
有两个模型

138
00:03:55,333 --> 00:03:57,366
所以一共是开源的6个大模型

139
00:03:57,366 --> 00:03:59,200
那这里面有很多相关的效果

140
00:03:59,200 --> 00:04:00,933
大家也可以重点的去看一看

141
00:04:00,933 --> 00:04:03,133
那很多其实这篇文章的内容

142
00:04:03,133 --> 00:04:04,533
zomi已经摘录

143
00:04:04,533 --> 00:04:06,200
成为具体的PPT的内容

144
00:04:06,450 --> 00:04:08,933
现在我们回到我们的胶片里面

145
00:04:08,933 --> 00:04:09,683
看看胶片

146
00:04:09,683 --> 00:04:11,733
或者看一下我们的这篇技术文章

147
00:04:11,733 --> 00:04:13,166
到底有哪些不一样

148
00:04:13,333 --> 00:04:14,850
看看这篇技术文章

149
00:04:14,850 --> 00:04:18,400
根据我们的Llama3.1里面的一些相关

150
00:04:18,400 --> 00:04:22,166
具体的细节有哪些比较有意思的点

151
00:04:23,166 --> 00:04:24,483
现在我们来到

152
00:04:24,600 --> 00:04:25,366
或者我们现在才

153
00:04:25,366 --> 00:04:26,883
正式的来到了第一个内容

154
00:04:26,883 --> 00:04:27,600
今天的视频

155
00:04:27,600 --> 00:04:28,483
可能会有点多

156
00:04:28,483 --> 00:04:29,200
所以有点长

157
00:04:29,200 --> 00:04:30,533
因为Llama3.1的解读

158
00:04:30,533 --> 00:04:32,000
还是你想深入点了解

159
00:04:32,000 --> 00:04:33,000
技术吗

160
00:04:33,000 --> 00:04:34,966
那还是原谅zomi有点长的内容

161
00:04:34,966 --> 00:04:35,366
第一个

162
00:04:35,366 --> 00:04:37,333
就是模型的性能了

163
00:04:37,333 --> 00:04:37,933
那首先

164
00:04:37,933 --> 00:04:39,083
我们做个简单总结

165
00:04:39,083 --> 00:04:41,200
Llama3.1 具体要点

166
00:04:41,366 --> 00:04:43,800
首先它提供了三个版本

167
00:04:43,850 --> 00:04:46,533
8B 70B 405B

168
00:04:46,533 --> 00:04:47,483
405B 其实对应

169
00:04:47,483 --> 00:04:48,200
405B 其实对应

170
00:04:48,200 --> 00:04:50,483
是接近一T的大模型

171
00:04:50,483 --> 00:04:51,366
所以说现在

172
00:04:51,366 --> 00:04:53,766
是核心为止最大的一个开源模型

173
00:04:53,766 --> 00:04:55,400
那405B的参数

174
00:04:55,400 --> 00:04:55,800
说实话

175
00:04:55,800 --> 00:04:57,933
部分的参数量或部分的性能

176
00:04:57,933 --> 00:05:00,483
其实已经超过了GPT-4大模型

177
00:05:00,483 --> 00:05:02,966
而且在微调的版本里面

178
00:05:02,966 --> 00:05:05,050
就使用了 SFT 跟 RLHF

179
00:05:05,050 --> 00:05:06,333
来进行一个对比

180
00:05:06,333 --> 00:05:06,733
实际上

181
00:05:06,733 --> 00:05:08,450
它使用了 SFT 跟 PPO

182
00:05:08,450 --> 00:05:09,483
没有使用 RLHF

183
00:05:09,483 --> 00:05:10,366
没有使用 RLHF

184
00:05:10,366 --> 00:05:12,166
其实还是有点区别的

185
00:05:12,166 --> 00:05:12,766
在后面

186
00:05:12,766 --> 00:05:13,333
将会介绍一下

187
00:05:13,333 --> 00:05:14,766
它的后处理是怎么实现

188
00:05:14,766 --> 00:05:15,400
那另外的话

189
00:05:15,400 --> 00:05:17,200
它引入了一个长向下文

190
00:05:17,200 --> 00:05:18,166
也就是长文本

191
00:05:18,250 --> 00:05:19,883
现在不提自己是长文本

192
00:05:20,083 --> 00:05:21,366
或对程序的进行处理

193
00:05:21,366 --> 00:05:23,366
说实话都说不过去了

194
00:05:23,366 --> 00:05:24,050
那另外的话

195
00:05:24,050 --> 00:05:26,800
它还支持多语言的输出

196
00:05:26,800 --> 00:05:28,483
还有一个多模态的输出

197
00:05:28,483 --> 00:05:29,850
而且推理的能力

198
00:05:29,966 --> 00:05:31,133
另外用了GQA

199
00:05:31,133 --> 00:05:31,883
推理的性能

200
00:05:31,883 --> 00:05:33,850
也是比较突出

201
00:05:34,250 --> 00:05:36,133
那在整个大模型里面

202
00:05:36,133 --> 00:05:37,333
他现在

203
00:05:37,333 --> 00:05:40,050
整个Llama3.1的一个405B

204
00:05:40,050 --> 00:05:41,283
整个模型的性能

205
00:05:41,283 --> 00:05:43,483
确实已经越来越接近了

206
00:05:43,483 --> 00:05:47,000
GPT4这么一个具体的效果

207
00:05:47,000 --> 00:05:47,650
那效果

208
00:05:47,650 --> 00:05:49,200
说实话我们不怀疑

209
00:05:49,366 --> 00:05:50,600
Llama 3.1的效果

210
00:05:50,600 --> 00:05:53,083
说实话应该是没有太多人去怀疑

211
00:05:53,083 --> 00:05:55,850
毕竟当年套壳Llama

212
00:05:55,850 --> 00:05:56,933
太多太多了

213
00:05:56,933 --> 00:05:58,600
包括灵异万物当时也被吐槽

214
00:05:58,800 --> 00:05:59,650
那可以看到

215
00:05:59,650 --> 00:06:00,533
下面这一条

216
00:06:00,533 --> 00:06:02,333
就是开源的大模型效果

217
00:06:02,450 --> 00:06:03,083
越来越好

218
00:06:03,083 --> 00:06:03,650
上面这个

219
00:06:03,650 --> 00:06:04,933
就是闭源的大模型效果

220
00:06:04,933 --> 00:06:06,733
其实发现了开源跟闭源

221
00:06:06,733 --> 00:06:08,850
其实越来越趋同

222
00:06:08,850 --> 00:06:09,933
所以现在来看到

223
00:06:09,933 --> 00:06:11,050
整个Llama3的系列

224
00:06:11,050 --> 00:06:12,566
已经越来越好

225
00:06:12,566 --> 00:06:12,933
而且

226
00:06:12,933 --> 00:06:15,933
也有很多人说Llama3真的牛逼

227
00:06:16,483 --> 00:06:18,850
既然Llama3.1这么牛逼

228
00:06:18,850 --> 00:06:20,850
那我们现在是不是重点去看看

229
00:06:20,850 --> 00:06:23,733
Llama3.1模型技术分解

230
00:06:23,766 --> 00:06:25,800
首先我们会从几个内容来看

231
00:06:25,800 --> 00:06:27,333
第一个是数据

232
00:06:27,333 --> 00:06:28,483
第二个是模型结构

233
00:06:28,483 --> 00:06:29,566
第三个预训练

234
00:06:29,800 --> 00:06:32,450
还有后训练相关内容

235
00:06:32,450 --> 00:06:33,283
那这里面内容

236
00:06:33,283 --> 00:06:34,333
可能会有点复杂

237
00:06:34,333 --> 00:06:34,933
有点长

238
00:06:34,933 --> 00:06:35,250
这里面

239
00:06:35,250 --> 00:06:38,083
我们马上就开始真正内容了

240
00:06:39,083 --> 00:06:42,366
首先第一个就是看训练的具体的数据

241
00:06:42,366 --> 00:06:44,133
说实话整个数据

242
00:06:44,133 --> 00:06:45,850
其实分为预训练和微调

243
00:06:45,850 --> 00:06:47,083
也就是后训练的过程当中

244
00:06:47,083 --> 00:06:47,566
一共使用了15万亿对 Token 来去实现的

245
00:06:47,566 --> 00:06:50,400
一共使用了15万亿对 Token 来去实现的


246
00:06:50,400 --> 00:06:52,600
而且整个数据配比

247
00:06:52,883 --> 00:06:54,450
是用了很多种不同数据

248
00:06:54,450 --> 00:06:55,166
不是一种数据

249
00:06:55,166 --> 00:06:57,400
而且是多语言就有很多种输入

250
00:06:57,400 --> 00:06:58,600
那在微调的过程中

251
00:06:58,600 --> 00:07:01,000
更多的是用了超过2,500万个

252
00:07:01,000 --> 00:07:02,966
综合合成的一个具体的数据

253
00:07:02,966 --> 00:07:03,333
然后

254
00:07:03,333 --> 00:07:06,083
通过SFT来去做具体实现

255
00:07:06,083 --> 00:07:07,883
而在数据处理过程当中

256
00:07:07,883 --> 00:07:09,133
说实话数据处理

257
00:07:09,133 --> 00:07:10,566
我们刚才很多数据

258
00:07:10,766 --> 00:07:12,366
是依赖于高质量数据

259
00:07:12,366 --> 00:07:13,883
因此处理的数据时候

260
00:07:13,883 --> 00:07:15,250
用了很多BERT的类

261
00:07:15,250 --> 00:07:16,766
网络模型例如Roberta

262
00:07:16,766 --> 00:07:17,966
还有distil Roberta

263
00:07:17,966 --> 00:07:20,200
还有fasttext相关的网络模型

264
00:07:20,200 --> 00:07:21,650
对我们的数据进行处理

265
00:07:21,650 --> 00:07:23,600
得到我们高质量的数据

266
00:07:23,600 --> 00:07:24,366
那这种方式

267
00:07:24,366 --> 00:07:25,966
就是保证我们数据安全

268
00:07:25,966 --> 00:07:28,133
还有提升我们数据质量

269
00:07:28,166 --> 00:07:29,533
那在微调场景

270
00:07:29,533 --> 00:07:31,566
说实话我们真正提出来

271
00:07:31,566 --> 00:07:33,166
是一个405B网络模型

272
00:07:33,166 --> 00:07:36,483
但是他其实同时也发布

273
00:07:36,483 --> 00:07:38,650
一个70B和8B网络模型

274
00:07:38,650 --> 00:07:39,733
那这两个网络模型

275
00:07:39,733 --> 00:07:42,733
其实是从405B教师模型里面

276
00:07:42,766 --> 00:07:44,483
蒸馏出两个小模型

277
00:07:44,483 --> 00:07:45,733
所以说可以看到

278
00:07:45,766 --> 00:07:47,366
小模型的效果也是越来越好

279
00:07:47,366 --> 00:07:48,850
是因为他充分的吸收

280
00:07:48,850 --> 00:07:52,200
教师模型相关知识

281
00:07:53,050 --> 00:07:54,366
那了解完我们数据之后

282
00:07:54,366 --> 00:07:55,400
我们现在来看看

283
00:07:55,400 --> 00:07:58,400
网络模型的具体架构

284
00:07:59,133 --> 00:08:01,283
在Llama3.0这个网络模型里面

285
00:08:01,283 --> 00:08:02,566
规划

286
00:08:02,566 --> 00:08:03,683
用了RMSNorm

287
00:08:03,683 --> 00:08:04,733
用了RMSNorm

288
00:08:04,733 --> 00:08:07,333
然后位置是Pre-Norm方式

289
00:08:07,333 --> 00:08:09,250
也就放在一开始前面

290
00:08:09,450 --> 00:08:10,400
那整个attention

291
00:08:10,400 --> 00:08:11,650
就是我们的自注意机制

292
00:08:11,800 --> 00:08:13,400
是使用了GQA

293
00:08:13,533 --> 00:08:15,966
整个embending是用了ROPE

294
00:08:15,966 --> 00:08:17,850
然后我们的激活

295
00:08:17,850 --> 00:08:18,483
FFN

296
00:08:18,483 --> 00:08:22,050
用了switch glue相关激活函数

297
00:08:22,050 --> 00:08:24,250
那整个Transformer结构已经介绍完

298
00:08:24,450 --> 00:08:26,483
整体它的网络模型架构

299
00:08:26,483 --> 00:08:27,566
用了注意力机制

300
00:08:27,566 --> 00:08:28,050
掩码

301
00:08:28,050 --> 00:08:29,600
Attention Mask 这种方式

302
00:08:29,600 --> 00:08:31,366
防止我们同一个序列不同完档之间

303
00:08:31,366 --> 00:08:33,133
会出现一个自注意力情况

304
00:08:33,200 --> 00:08:34,083
这种方式

305
00:08:34,083 --> 00:08:36,250
对于长序列训练非常有效

306
00:08:36,250 --> 00:08:38,533
而且在整个技术文档里面分析

307
00:08:38,533 --> 00:08:40,200
它最大化的用了一个 Scaling law

308
00:08:40,200 --> 00:08:41,600
算力比

309
00:08:41,800 --> 00:08:45,250
也就是为什么整个Llama405B时候

310
00:08:45,250 --> 00:08:46,966
采用是一个标准全双板架构

311
00:08:46,966 --> 00:08:48,133
而且引入了GQA

312
00:08:48,133 --> 00:08:50,966
而不是现在大家都用的比较流行

313
00:08:50,966 --> 00:08:53,366
或者MOE架构

314
00:08:53,883 --> 00:08:55,283
是因为Meta的研究员

315
00:08:55,283 --> 00:08:58,766
就觉得这种标准的Transformer结构

316
00:08:58,766 --> 00:09:00,083
它更加稳定

317
00:09:00,083 --> 00:09:02,600
或者更加有利于稳定的去训练

318
00:09:02,766 --> 00:09:03,883
因此整个网络模型

319
00:09:03,883 --> 00:09:05,200
就像下面所设

320
00:09:05,200 --> 00:09:06,250
输的是一个 Token

321
00:09:06,250 --> 00:09:08,083
通过一个embedding的模块之后

322
00:09:08,083 --> 00:09:10,450
交给很多个Transformer的结构

323
00:09:10,450 --> 00:09:12,450
那这个就是一个Transformer的结构

324
00:09:12,450 --> 00:09:14,283
然后这是一个Transformer结构

325
00:09:14,283 --> 00:09:15,250
不断的堆叠

326
00:09:15,250 --> 00:09:16,733
然后输出我们的 Tokens

327
00:09:16,733 --> 00:09:17,200
完成了

328
00:09:17,200 --> 00:09:19,650
这么整个一个网络模型的加固的输出

329
00:09:19,650 --> 00:09:20,966
那我们可以看到刚才里面

330
00:09:20,966 --> 00:09:22,766
其实还有几个细节

331
00:09:22,766 --> 00:09:25,566
第一个就是分组的查询注意力机制

332
00:09:25,566 --> 00:09:26,850
也就是GQA

333
00:09:29,083 --> 00:09:31,000
就说明了它的一个GQA里面

334
00:09:31,000 --> 00:09:33,683
用了8个头儿或者8个KV对应

335
00:09:33,683 --> 00:09:35,400
然后提升了整个推理性能

336
00:09:35,400 --> 00:09:37,450
并且减少了我们在解码时的收获

337
00:09:37,450 --> 00:09:39,650
在推理时候解码的KV Cache

338
00:09:40,083 --> 00:09:42,366
那另外一个很注意的点就是 Vocabulary size

339
00:09:42,366 --> 00:09:44,200
我们的词表的大小

340
00:09:44,200 --> 00:09:45,850
一共有128K

341
00:09:45,850 --> 00:09:46,850
对比起Llama2

342
00:09:46,850 --> 00:09:48,166
它同时提升

343
00:09:48,166 --> 00:09:50,333
语料压缩比

344
00:09:50,333 --> 00:09:52,083
更好支持我们更多语言

345
00:09:52,083 --> 00:09:54,600
另外的号位置编码 Positional Embeddings

346
00:09:54,600 --> 00:09:56,933
其实里面有一条很重要的参数

347
00:09:56,933 --> 00:09:57,800
特别的说明

348
00:09:57,850 --> 00:09:59,200
等于50万

349
00:09:59,200 --> 00:10:00,650
表示整个Positional Embeddings

350
00:10:00,733 --> 00:10:03,283
其实也更好去支持上下文

351
00:10:03,733 --> 00:10:04,733
了解完相关模型

352
00:10:04,733 --> 00:10:05,133
架构之后

353
00:10:05,133 --> 00:10:06,733
我们看一下具体

354
00:10:06,733 --> 00:10:09,850
刚才讲到attention结构里面细节

355
00:10:09,850 --> 00:10:10,450
实际上

356
00:10:10,450 --> 00:10:13,083
里面或者我们一般结构

357
00:10:13,366 --> 00:10:15,050
Llama2 70B

358
00:10:15,050 --> 00:10:17,050
用实际上是一个MHA

359
00:10:17,050 --> 00:10:18,800
也就是我们 Key values Queries

360
00:10:18,800 --> 00:10:20,250
都是一一对应

361
00:10:20,283 --> 00:10:21,050
那后来

362
00:10:21,050 --> 00:10:22,800
就提出了一个MQA

363
00:10:22,800 --> 00:10:24,650
就是Multi Queries attention

364
00:10:24,650 --> 00:10:27,650
通过一个KV去辐射很多个Queries

365
00:10:27,650 --> 00:10:28,200
那后来

366
00:10:28,200 --> 00:10:28,933
大家觉得嗯

367
00:10:28,933 --> 00:10:31,200
我们可以通过一个Group Queries Attention

368
00:10:31,200 --> 00:10:32,600
然后多个KB

369
00:10:32,600 --> 00:10:34,366
对多个Queries

370
00:10:34,366 --> 00:10:35,250
那这种方式

371
00:10:35,250 --> 00:10:36,366
在整个GQA

372
00:10:36,366 --> 00:10:37,766
能够取得一个比较好

373
00:10:37,766 --> 00:10:38,800
一个性能跟内存

374
00:10:38,800 --> 00:10:40,566
一个机体收益

375
00:10:41,166 --> 00:10:43,533
那了解完整个网络模型结构之后

376
00:10:43,533 --> 00:10:45,683
我们现在来到了另外一个内容了

377
00:10:45,683 --> 00:10:48,133
就看整个网络模型预训练过程

378
00:10:48,133 --> 00:10:50,333
特别是pretuning整体是怎么样

379
00:10:50,333 --> 00:10:52,683
说实话整个预训练时候

380
00:10:52,683 --> 00:10:53,566
整个论文

381
00:10:53,566 --> 00:10:55,483
说实话或者他一个技术文章

382
00:10:55,483 --> 00:10:57,600
就说了他用了好几个步骤

383
00:10:57,600 --> 00:10:58,766
第一个步骤

384
00:10:58,766 --> 00:10:59,933
就是把我们数据

385
00:10:59,933 --> 00:11:01,283
做了一个很好配比

386
00:11:01,283 --> 00:11:03,250
之后就开始进行一个预训练了

387
00:11:03,566 --> 00:11:04,000
所以这里面

388
00:11:04,000 --> 00:11:07,083
叫做initia pre Training这么一个内容

389
00:11:07,166 --> 00:11:08,283
那这种方式

390
00:11:08,283 --> 00:11:10,166
其实跟传统没有太多区别

391
00:11:10,333 --> 00:11:11,250
运行完之后

392
00:11:11,250 --> 00:11:11,883
接下一个

393
00:11:11,883 --> 00:11:13,366
就是长上下文

394
00:11:13,366 --> 00:11:14,883
也就是长文本一个序列

395
00:11:14,883 --> 00:11:16,450
longer context这种序列了

396
00:11:16,450 --> 00:11:18,933
那这种方式说实话比较有特别

397
00:11:18,933 --> 00:11:20,600
因为我们序数据或者语料

398
00:11:20,600 --> 00:11:21,483
它是按等级

399
00:11:21,483 --> 00:11:21,883
因此

400
00:11:21,883 --> 00:11:25,200
我们从8K Tokens到128K Tokens里面

401
00:11:25,200 --> 00:11:27,083
分开了六个阶段

402
00:11:27,083 --> 00:11:28,050
六个档次

403
00:11:28,050 --> 00:11:29,683
逐步去增加我们数据

404
00:11:29,683 --> 00:11:32,566
然后最后来拓展到128K  Token 

405
00:11:32,566 --> 00:11:34,333
而不是全部打散进行重来

406
00:11:34,333 --> 00:11:36,166
或者全部打散给到我们网络模型

407
00:11:36,166 --> 00:11:37,283
进行一个预训练

408
00:11:37,400 --> 00:11:38,733
大家要注意第二个点

409
00:11:38,733 --> 00:11:40,533
它分档次来进行训练

410
00:11:40,533 --> 00:11:41,333
很有意思

411
00:11:42,000 --> 00:11:43,483
第三个训练过程

412
00:11:43,483 --> 00:11:45,483
叫做退火Annealing

413
00:11:45,533 --> 00:11:47,733
在整个训练最后

414
00:11:47,733 --> 00:11:49,566
4,000万个 Tokens步骤时候

415
00:11:49,566 --> 00:11:50,683
线性将学习率

416
00:11:50,683 --> 00:11:51,650
Annealing到0

417
00:11:51,650 --> 00:11:52,733
也就退火到0了

418
00:11:52,733 --> 00:11:56,400
同时保持上下文是128K Tokens时候

419
00:11:56,683 --> 00:11:59,366
调整了整个数据一个具体配比

420
00:11:59,366 --> 00:12:01,133
特别是增加了一个数理逻辑

421
00:12:01,133 --> 00:12:02,533
代码相关内容

422
00:12:02,600 --> 00:12:04,533
那最后退火完之后

423
00:12:04,533 --> 00:12:05,250
因为第四步

424
00:12:05,250 --> 00:12:06,766
其实跟退火是强相关

425
00:12:07,050 --> 00:12:07,483
退火期间

426
00:12:07,483 --> 00:12:10,800
得到好几个不同模型权重

427
00:12:10,800 --> 00:12:11,683
求一个均值

428
00:12:11,683 --> 00:12:12,000
最后

429
00:12:12,000 --> 00:12:14,650
得到我们退火后一个模型输出

430
00:12:14,650 --> 00:12:16,566
因此在整个训练过程中

431
00:12:16,566 --> 00:12:18,200
主要是分开三个内容

432
00:12:18,200 --> 00:12:19,050
第一个就输入

433
00:12:19,050 --> 00:12:20,083
第二个是长文本

434
00:12:20,083 --> 00:12:21,483
第三个就退火

435
00:12:21,600 --> 00:12:23,533
然后用退火好几个模型求个均值

436
00:12:23,533 --> 00:12:24,683
再进行输出

437
00:12:24,683 --> 00:12:25,283
这看到了

438
00:12:25,283 --> 00:12:27,683
整个训练过程还是比较复杂

439
00:12:28,333 --> 00:12:29,083
这里面

440
00:12:29,083 --> 00:12:30,850
悄悄跟大家说个事

441
00:12:31,333 --> 00:12:33,000
就是如果你现在

442
00:12:33,000 --> 00:12:35,000
还在一些国内大模型厂商

443
00:12:35,000 --> 00:12:36,050
里面训不出来

444
00:12:36,050 --> 00:12:37,283
你赶紧过来看看

445
00:12:37,283 --> 00:12:39,533
这篇文章还是很值得我们去参考

446
00:12:40,200 --> 00:12:41,166
现在我们怀疑到

447
00:12:41,166 --> 00:12:42,800
整个训练其他内容

448
00:12:42,800 --> 00:12:44,450
里面在整个训练过程当中

449
00:12:44,450 --> 00:12:47,083
Mate就用了1.6万张GPU H100

450
00:12:47,083 --> 00:12:49,566
训练 405B这个大模型

451
00:12:49,566 --> 00:12:51,133
重点还考虑两个事情

452
00:12:51,133 --> 00:12:52,333
第一个就是并行策略

453
00:12:52,333 --> 00:12:53,650
第二是故障率

454
00:12:53,650 --> 00:12:55,283
那并行策略里面

455
00:12:55,283 --> 00:12:56,600
其实采用了一个4D并行

456
00:12:56,600 --> 00:12:57,566
也就是说张量并行

457
00:12:57,566 --> 00:12:58,933
流水并行还有上下文并行

458
00:12:58,933 --> 00:13:01,933
和数据并行四个内容

459
00:13:02,333 --> 00:13:03,766
把这些都用上之后

460
00:13:03,766 --> 00:13:06,250
在一个混合精度BF16下面

461
00:13:06,283 --> 00:13:10,933
整个GPU集群算力利用率到了38%-41%

462
00:13:11,166 --> 00:13:13,366
所以大家一定要了解或理解

463
00:13:13,366 --> 00:13:15,133
这个集群算力

464
00:13:15,133 --> 00:13:16,133
利用率

465
00:13:16,133 --> 00:13:16,933
实际上

466
00:13:17,083 --> 00:13:19,683
英伟达也没有那么高

467
00:13:19,683 --> 00:13:21,333
别吹国内特别差

468
00:13:21,566 --> 00:13:23,450
英伟达利用率能够去到百分之五

469
00:13:23,450 --> 00:13:25,883
六十这个我听过很多人去吹

470
00:13:25,883 --> 00:13:27,450
或者很多人去故意黑

471
00:13:27,533 --> 00:13:28,450
嗯没有必要

472
00:13:28,450 --> 00:13:31,683
英伟达在一个16万卡集群里面

473
00:13:31,850 --> 00:13:36,050
那它一个BF16MFU其实也并不高

474
00:13:36,566 --> 00:13:39,650
那接下来我们看一下它整个4D并行了

475
00:13:39,650 --> 00:13:40,333
可以看到

476
00:13:40,333 --> 00:13:41,133
CP跟TP

477
00:13:41,133 --> 00:13:43,733
其实在我们单节点里面去进行

478
00:13:43,733 --> 00:13:45,133
而PP一般来说

479
00:13:45,133 --> 00:13:47,283
因为不会跨机组太远了

480
00:13:47,566 --> 00:13:49,166
所以在MATA训练过程当中

481
00:13:49,166 --> 00:13:50,933
一般来说是跨两台机器

482
00:13:50,933 --> 00:13:51,650
更多

483
00:13:51,650 --> 00:13:54,133
其他就通过DP进行一个扩展了

484
00:13:54,533 --> 00:13:55,400
那我们现在

485
00:13:55,400 --> 00:13:56,766
回到这一个表里面

486
00:13:56,766 --> 00:13:57,683
可以看到

487
00:13:57,683 --> 00:13:59,400
TP我们基本上都是设为

488
00:13:59,400 --> 00:14:00,566
8也就是单机8行

489
00:14:00,566 --> 00:14:01,533
里面去跑

490
00:14:01,566 --> 00:14:03,766
TP都会设成16

491
00:14:04,000 --> 00:14:04,800
就是我们基本上

492
00:14:04,800 --> 00:14:07,450
就会跨两台机组里面去实现

493
00:14:07,450 --> 00:14:09,083
然后整个DP

494
00:14:09,083 --> 00:14:10,166
拓展非常大

495
00:14:10,166 --> 00:14:11,933
但是随着我们Seq. Len.变长了

496
00:14:11,933 --> 00:14:13,000
我们DP

497
00:14:13,000 --> 00:14:15,450
实际上是在减少Batch size数量了

498
00:14:15,450 --> 00:14:16,683
其实并没有怎么变化

499
00:14:16,683 --> 00:14:17,533
所以大家可以看一下

500
00:14:17,533 --> 00:14:20,083
相关并行配置组合

501
00:14:20,083 --> 00:14:22,083
对我们整个一个集群训练

502
00:14:22,083 --> 00:14:23,533
一个具体影响

503
00:14:24,050 --> 00:14:25,650
那了解完整个4D并行以外

504
00:14:25,650 --> 00:14:27,933
我们可以看到它一个PP并行

505
00:14:27,933 --> 00:14:29,800
用是一个1F1B方式

506
00:14:29,800 --> 00:14:30,766
所谓1F1B

507
00:14:30,766 --> 00:14:31,966
one forward

508
00:14:31,966 --> 00:14:33,566
one backward那怎么去看出来

509
00:14:33,566 --> 00:14:35,533
就因为我们forward之后

510
00:14:35,533 --> 00:14:36,600
就马上执行一个backward

511
00:14:36,600 --> 00:14:38,133
forward之后马上执行一个backward

512
00:14:38,250 --> 00:14:39,650
基本上都是一一配比

513
00:14:41,933 --> 00:14:43,883
非常复杂一个流水

514
00:14:43,883 --> 00:14:44,800
并行策略

515
00:14:44,800 --> 00:14:47,650
是使用一个比较naive方式

516
00:14:47,766 --> 00:14:49,566
那在整个刚才讲到了

517
00:14:49,566 --> 00:14:50,933
其实除了并行以外

518
00:14:51,166 --> 00:14:53,683
还有另外一个内容非常值得注意

519
00:14:53,683 --> 00:14:56,883
就是整个训练故障率了

520
00:14:56,883 --> 00:14:57,800
那这个故障率

521
00:14:57,800 --> 00:15:00,083
会非常考验我们整个集群

522
00:15:00,083 --> 00:15:01,566
一个可用度

523
00:15:01,566 --> 00:15:03,683
在Llama3.10训练过程当中

524
00:15:03,683 --> 00:15:04,483
说实话

525
00:15:04,600 --> 00:15:08,933
有90%一个具体有效训练时间

526
00:15:08,933 --> 00:15:11,533
那这个90%有效训练时间

527
00:15:11,533 --> 00:15:13,683
就意味着一共在54天内

528
00:15:13,683 --> 00:15:17,333
就是3,930万个GPU小时里面

529
00:15:17,366 --> 00:15:19,883
在1.6万卡H100集群当中

530
00:15:19,883 --> 00:15:22,800
每天至少都有一次中断了

531
00:15:23,283 --> 00:15:25,883
在整个中断统计里面可以看到

532
00:15:25,933 --> 00:15:27,650
在不到两个月时间过程当中

533
00:15:27,650 --> 00:15:28,800
一共中断次数

534
00:15:28,800 --> 00:15:30,533
有419次

535
00:15:30,533 --> 00:15:32,366
其中确认快了怀疑跟硬件相关

536
00:15:32,366 --> 00:15:33,683
就GPU相关呃

537
00:15:33,683 --> 00:15:35,683
高达78%

538
00:15:35,683 --> 00:15:37,450
我们可以看到英伟达集群

539
00:15:37,450 --> 00:15:38,600
其实也有很多问题

540
00:15:38,600 --> 00:15:41,166
现在特斯拉说组一个10万卡集群

541
00:15:41,166 --> 00:15:43,083
说实话他只是组一个10万卡集群

542
00:15:43,083 --> 00:15:44,566
但是他跑一个大模型

543
00:15:44,800 --> 00:15:45,933
绝对不是一个大模型

544
00:15:45,933 --> 00:15:47,566
在10万卡上面去跑

545
00:15:47,566 --> 00:15:49,883
现在没有一个业界非常成熟案例

546
00:15:49,883 --> 00:15:51,566
能够真正把一个大模型

547
00:15:51,566 --> 00:15:52,883
跑在一个10万卡

548
00:15:52,966 --> 00:15:55,283
在一个1.6万卡或者两两万多

549
00:15:55,283 --> 00:15:56,133
3万多卡以

550
00:15:56,133 --> 00:15:59,050
其实已经是现在GPU集群封顶了

551
00:15:59,333 --> 00:16:00,000
那但是

552
00:16:00,000 --> 00:16:02,200
其实那篇文章或者这篇文章也说到了

553
00:16:02,200 --> 00:16:04,600
因为整个集群自动化运维效果

554
00:16:04,600 --> 00:16:07,283
或者相关软件站设计比较好

555
00:16:07,283 --> 00:16:09,566
所以整体故障即使次数很多

556
00:16:09,566 --> 00:16:10,966
到了419次

557
00:16:11,166 --> 00:16:12,800
但是在整个过程当中

558
00:16:12,800 --> 00:16:13,683
只有三次故障

559
00:16:13,683 --> 00:16:15,333
是需要人为手段干预

560
00:16:15,333 --> 00:16:16,250
大部分工作

561
00:16:16,250 --> 00:16:17,800
其实还是直接自动化

562
00:16:17,800 --> 00:16:20,766
重新帮你做一个真正拉起

563
00:16:21,566 --> 00:16:22,850
了解完训练内容之后

564
00:16:22,850 --> 00:16:24,250
我们现在来看一下

565
00:16:24,283 --> 00:16:26,650
后训练相关一个环节

566
00:16:26,650 --> 00:16:27,683
叫post training了

567
00:16:27,683 --> 00:16:29,166
那整个后训练过程当中

568
00:16:29,166 --> 00:16:30,933
其实里面用到了

569
00:16:30,933 --> 00:16:31,966
嗯怎么说

570
00:16:31,966 --> 00:16:32,966
刚才预训练

571
00:16:32,966 --> 00:16:34,766
它其实提供了一个原声模型

572
00:16:34,766 --> 00:16:35,600
instruct模型

573
00:16:35,600 --> 00:16:37,800
是通过我们后训练去提供

574
00:16:37,933 --> 00:16:39,283
那这里面就分开三个步骤

575
00:16:39,283 --> 00:16:40,283
一个是SFT

576
00:16:40,283 --> 00:16:41,966
一个是拒绝采样

577
00:16:41,966 --> 00:16:43,533
一个是DPO

578
00:16:43,683 --> 00:16:45,133
这面点打错字了

579
00:16:45,283 --> 00:16:46,050
那三个过程当中

580
00:16:46,050 --> 00:16:48,283
我们逐个过程的去打开

581
00:16:48,283 --> 00:16:49,683
第一个就是CSFT

582
00:16:49,683 --> 00:16:51,050
就训练我们的 Reward model

583
00:16:51,050 --> 00:16:52,000
用了一系列

584
00:16:52,000 --> 00:16:53,683
人工标注好一些数据

585
00:16:54,000 --> 00:16:55,883
训练我们整个 Reward model

586
00:16:55,883 --> 00:16:56,966
用来评价

587
00:16:56,966 --> 00:16:58,850
整个Prompts跟answer一个数据对

588
00:16:58,850 --> 00:17:01,083
也就是我们指令微调后数据对

589
00:17:01,083 --> 00:17:02,083
那第二个内容

590
00:17:02,083 --> 00:17:03,366
就是拒绝采样了

591
00:17:03,366 --> 00:17:04,600
因为刚才第一步时候

592
00:17:04,600 --> 00:17:06,683
已经得到了一个 Reward model了

593
00:17:06,766 --> 00:17:07,250
那现在

594
00:17:07,250 --> 00:17:10,283
我们针对一个 Reward model来对输入Prompts

595
00:17:10,566 --> 00:17:12,400
或者输入指令

596
00:17:12,933 --> 00:17:14,683
通过之前一个大模型

597
00:17:14,683 --> 00:17:16,283
输出几个若干

598
00:17:16,283 --> 00:17:17,133
然后用 Reward model

599
00:17:17,133 --> 00:17:19,083
来去评价我们这些答案好

600
00:17:19,083 --> 00:17:19,800
还是不好

601
00:17:19,800 --> 00:17:21,333
那选择得分比较好

602
00:17:21,333 --> 00:17:23,650
作为我们SFT一个具体数据

603
00:17:23,650 --> 00:17:26,083
因此这个叫做我们拒绝采样

604
00:17:26,083 --> 00:17:27,200
也就是 Rejection simple

605
00:17:27,200 --> 00:17:30,366
我们简称RS相关内容

606
00:17:30,366 --> 00:17:31,800
那么现在来看看最后一个

607
00:17:31,800 --> 00:17:33,766
就是DPO训练了

608
00:17:33,766 --> 00:17:36,083
那人工标注好一些数据

609
00:17:36,083 --> 00:17:37,200
就会给到我们

610
00:17:37,200 --> 00:17:38,366
一个DPO网络模型

611
00:17:38,366 --> 00:17:40,483
去调整整个模型参数

612
00:17:40,483 --> 00:17:41,400
那这些参数话

613
00:17:41,400 --> 00:17:42,400
这些数据怎么来了

614
00:17:42,400 --> 00:17:43,566
是通过我们刚才

615
00:17:43,566 --> 00:17:45,566
一个拒绝采样数据来实现

616
00:17:45,566 --> 00:17:46,850
那整个DPU本质

617
00:17:46,850 --> 00:17:47,800
其实是一个二分类

618
00:17:47,800 --> 00:17:49,483
从人工标注数据里面

619
00:17:49,483 --> 00:17:50,850
去学习调整出

620
00:17:50,850 --> 00:17:52,333
并且鼓励我们网络模型

621
00:17:52,333 --> 00:17:53,966
输出一个比较积极向上

622
00:17:53,966 --> 00:17:55,333
一个具体答案

623
00:17:55,333 --> 00:17:58,650
因此后训练分开三个步骤

624
00:17:58,650 --> 00:17:59,250
那其实

625
00:17:59,250 --> 00:18:01,533
上面三个步骤会不断去反馈迭代

626
00:18:01,533 --> 00:18:02,933
然后使得我们网络模型

627
00:18:02,933 --> 00:18:03,800
越来越好

628
00:18:03,800 --> 00:18:05,533
大家如果想深入去了解他

629
00:18:05,533 --> 00:18:07,283
刚才所有线段轮回了

630
00:18:07,283 --> 00:18:09,283
也可以重点看看这里面内容

631
00:18:09,800 --> 00:18:10,966
那终于在这里面

632
00:18:10,966 --> 00:18:11,766
就要提一个点了

633
00:18:11,766 --> 00:18:12,966
就是 Reward model

634
00:18:12,966 --> 00:18:15,050
在后训练过程当中一个具体区别

635
00:18:15,166 --> 00:18:16,133
说实话这里面

636
00:18:16,133 --> 00:18:18,366
说它跟Chat GPT里面一个最大区别

637
00:18:18,366 --> 00:18:20,400
就是chat GPT用是RLHF

638
00:18:22,683 --> 00:18:24,400
真正是通过RM

639
00:18:24,400 --> 00:18:26,533
就是 Reward model这个网络模型

640
00:18:26,800 --> 00:18:28,483
整个RM网络模型打分

641
00:18:28,483 --> 00:18:30,400
是作用在PPO强化学习阶段

642
00:18:30,400 --> 00:18:31,966
而Llama3.1已用RM

643
00:18:31,966 --> 00:18:34,566
去筛选出高质量SFT数据

644
00:18:34,566 --> 00:18:38,766
所以它们RM Reward model作用是不一样

645
00:18:39,250 --> 00:18:40,050
另外的话

646
00:18:40,050 --> 00:18:41,650
里面SFT数据

647
00:18:41,650 --> 00:18:42,883
其实是通过合成数据

648
00:18:42,883 --> 00:18:44,683
因为我们通过一个RS采样

649
00:18:44,683 --> 00:18:48,250
 rejection simpling过程了去回答

650
00:18:48,250 --> 00:18:50,400
那然后再通过 Reward model

651
00:18:50,400 --> 00:18:52,283
去把选择最好的一个答案

652
00:18:52,283 --> 00:18:53,050
因此

653
00:18:53,050 --> 00:18:56,850
里面合成的数据用来训练SFT

654
00:18:56,933 --> 00:18:57,450
因此

655
00:18:57,450 --> 00:18:59,283
真正的是采用一个合成的数据

656
00:18:59,283 --> 00:19:02,283
来训练我们的一个网络模型的

657
00:19:02,600 --> 00:19:04,250
在整个后训练的过程当中

658
00:19:04,250 --> 00:19:05,850
为了提升我们下游任务

659
00:19:05,850 --> 00:19:07,200
一个具体性能了

660
00:19:07,200 --> 00:19:10,083
所以里面也采用了一个嗯

661
00:19:10,083 --> 00:19:11,966
采用了一个最简单DPO

662
00:19:11,966 --> 00:19:13,966
而不是非常复杂强化学习方式

663
00:19:13,966 --> 00:19:16,566
因为还是为了满足Scaling law了

664
00:19:16,566 --> 00:19:17,733
那我们可以看到这篇文章

665
00:19:17,733 --> 00:19:18,683
或者Llama3.1

666
00:19:18,683 --> 00:19:19,933
这个模型

667
00:19:19,933 --> 00:19:22,333
为了更好的把模型规模做大

668
00:19:22,333 --> 00:19:23,683
把数据量做大了

669
00:19:23,683 --> 00:19:24,683
他很多东西

670
00:19:24,683 --> 00:19:26,933
都是能够极简的就极简

671
00:19:26,966 --> 00:19:28,966
尽可能的把更多的心思

672
00:19:28,966 --> 00:19:30,200
放在我们的调参

673
00:19:30,200 --> 00:19:31,250
放在tricks上面

674
00:19:31,250 --> 00:19:33,533
放在我们数据处理里面

675
00:19:33,533 --> 00:19:34,600
那讲到数据处理了

676
00:19:34,600 --> 00:19:35,800
就来到了我们第二个内容

677
00:19:35,800 --> 00:19:37,800
就提供模型编码的能力

678
00:19:38,000 --> 00:19:38,733
真正的去引导

679
00:19:38,733 --> 00:19:41,933
或者创建更优质的一些相关的数据

680
00:19:41,933 --> 00:19:43,333
那在推理的环节当中

681
00:19:43,333 --> 00:19:45,250
其实其实整个技术文章里面

682
00:19:45,250 --> 00:19:47,933
也说了他用了FP8的一个具体量化

683
00:19:47,933 --> 00:19:49,966
将数的权重还有输入的数据

684
00:19:49,966 --> 00:19:51,000
量化成FP 8

685
00:19:51,000 --> 00:19:52,250
然后乘一个scale因子

686
00:19:52,283 --> 00:19:53,733
fp8乘fp8 就输出一个bf16

687
00:19:53,733 --> 00:19:56,600
使得整个推点速度变得更快

688
00:19:56,600 --> 00:19:57,933
显存占用更少

689
00:19:57,933 --> 00:19:59,533
那这是针对特定的下游任务

690
00:19:59,533 --> 00:20:00,766
一些具体的性能

691
00:20:00,766 --> 00:20:01,866
提升点

692
00:20:01,866 --> 00:20:03,733
那在最后

693
00:20:03,733 --> 00:20:05,333
也就是在整个技术文章

694
00:20:05,333 --> 00:20:07,133
其实 Result之后

695
00:20:07,133 --> 00:20:08,283
它还有一个大模型

696
00:20:08,283 --> 00:20:10,483
或者多模态加持

697
00:20:10,766 --> 00:20:11,400
那这里面

698
00:20:11,400 --> 00:20:14,800
我们真正要去聊一聊一些多模态

699
00:20:14,933 --> 00:20:16,083
因为刚才效果

700
00:20:16,083 --> 00:20:17,000
除了一个问答以外

701
00:20:17,000 --> 00:20:21,766
它可以加入图像视频语音相关能力

702
00:20:21,766 --> 00:20:22,283
那这里面

703
00:20:22,283 --> 00:20:23,400
我们重点来看看

704
00:20:23,400 --> 00:20:26,200
整个多模态encoder是怎么去处理

705
00:20:26,333 --> 00:20:26,966
那我们可以看到

706
00:20:26,966 --> 00:20:29,250
图像跟语音encoder实际上是分开

707
00:20:29,250 --> 00:20:30,683
我们看下面这个图

708
00:20:30,683 --> 00:20:31,883
会更清晰

709
00:20:31,883 --> 00:20:33,250
那左边这个模块

710
00:20:33,250 --> 00:20:35,050
就是图像一个encoder

711
00:20:35,083 --> 00:20:36,133
右边这个模块

712
00:20:36,133 --> 00:20:37,933
就是我们音频encoder

713
00:20:37,933 --> 00:20:39,050
那我们可以分开三个模块

714
00:20:39,050 --> 00:20:39,850
一个是图像

715
00:20:39,850 --> 00:20:40,566
一个视频

716
00:20:40,566 --> 00:20:41,683
一个是音频

717
00:20:42,333 --> 00:20:44,333
那在整个视觉里面

718
00:20:44,333 --> 00:20:45,800
包括我们图像和video

719
00:20:45,800 --> 00:20:47,883
它都会通过一个encoder网络模型

720
00:20:47,883 --> 00:20:49,200
把我们数据

721
00:20:49,250 --> 00:20:51,333
呃变成一些 Token 或者 patch

722
00:20:51,333 --> 00:20:51,883
然后

723
00:20:51,883 --> 00:20:54,733
接着就给到大语言模型里面

724
00:20:54,733 --> 00:20:57,650
通过cross attention进行一个交互

725
00:20:57,650 --> 00:20:59,200
就通过cross attention方式

726
00:20:59,450 --> 00:21:01,766
把一些图片或者视频的数据的特征

727
00:21:01,766 --> 00:21:04,483
加到我们的LLM大语言模型里面

728
00:21:04,533 --> 00:21:05,250
那另外的话

729
00:21:05,250 --> 00:21:08,133
我们看另外一个分支的就是语音

730
00:21:08,133 --> 00:21:09,166
在语音这个分支的

731
00:21:09,166 --> 00:21:11,083
用的是空方码这个模块

732
00:21:11,083 --> 00:21:12,850
Conformer这个格式

733
00:21:12,850 --> 00:21:14,483
或者里面的一个结构

734
00:21:14,483 --> 00:21:17,483
然后得到我们的一个语音的压缩

735
00:21:17,483 --> 00:21:18,366
编码的信息

736
00:21:18,366 --> 00:21:20,800
之后通过 Tokens embedding

737
00:21:20,933 --> 00:21:23,333
也就是把一些语言的数据

738
00:21:23,333 --> 00:21:26,366
跟我们的音频的数据concat到一起

739
00:21:26,366 --> 00:21:27,600
就绑在一条上面

740
00:21:27,600 --> 00:21:29,166
然后再给我们的LLM大语言

741
00:21:29,166 --> 00:21:31,000
模型进行一个训练

742
00:21:31,000 --> 00:21:33,766
最终输出我们的一个具体的结果

743
00:21:33,883 --> 00:21:34,933
那这里面可以看到

744
00:21:34,933 --> 00:21:37,333
语音跟图片的处理不一样

745
00:21:37,533 --> 00:21:39,733
而且从论文的一个位置可以看到

746
00:21:39,733 --> 00:21:41,366
它为什么放在  Post-Training之后

747
00:21:41,366 --> 00:21:43,766
是因为我先训练了这个大模型

748
00:21:44,483 --> 00:21:46,133
LLM这个训完大模型之后

749
00:21:46,133 --> 00:21:48,400
我再把图像的一个多模态的数据

750
00:21:48,400 --> 00:21:49,050
加进去

751
00:21:49,050 --> 00:21:51,533
再把音频的数据的多模态的信息

752
00:21:51,533 --> 00:21:54,600
加进去它是这么一个过程

753
00:21:55,533 --> 00:21:56,533
一口气讲了这么多

754
00:21:56,533 --> 00:21:57,533
真不容易

755
00:21:57,533 --> 00:21:59,133
希望大家给个三连

756
00:21:59,133 --> 00:21:59,733
那第三个

757
00:21:59,733 --> 00:22:01,800
就对业界的整体的思考

758
00:22:01,800 --> 00:22:04,083
说实话在整个

759
00:22:04,083 --> 00:22:04,766
现在大模型

760
00:22:04,766 --> 00:22:07,050
已经发展了有一一年多的时间

761
00:22:07,050 --> 00:22:09,083
我们看下整个大模型结构的演进

762
00:22:09,133 --> 00:22:11,400
说实话其实直到现在为止

763
00:22:11,400 --> 00:22:12,766
特别是幻方出来

764
00:22:12,966 --> 00:22:15,083
大家对于整个大模型的结构的选型

765
00:22:15,083 --> 00:22:16,400
其实还是摇摆不定的

766
00:22:16,400 --> 00:22:18,083
到底是选MOE的架构

767
00:22:18,083 --> 00:22:20,283
还是选Transformer这种稠密的架构

768
00:22:20,566 --> 00:22:22,483
说实话zomi觉得Moe的优势

769
00:22:22,483 --> 00:22:22,966
其实在于

770
00:22:22,966 --> 00:22:24,800
减少我们整个训练和推理

771
00:22:24,800 --> 00:22:26,166
一个具体的成本

772
00:22:26,166 --> 00:22:27,766
因为能够很好的去减少

773
00:22:27,766 --> 00:22:29,000
我们训练和推理的成本

774
00:22:29,000 --> 00:22:29,683
但是代价

775
00:22:29,683 --> 00:22:31,366
就是训练可能并不是很稳定

776
00:22:31,366 --> 00:22:32,566
不是所有的厂商

777
00:22:32,566 --> 00:22:33,933
都能够

778
00:22:33,933 --> 00:22:35,883
真正的训出一个很好效果

779
00:22:35,883 --> 00:22:36,883
一个MOE模型

780
00:22:36,883 --> 00:22:38,200
大部分都是

781
00:22:38,200 --> 00:22:39,133
或者幻方

782
00:22:39,133 --> 00:22:40,250
进行一个微调

783
00:22:40,250 --> 00:22:41,200
所以说这里面

784
00:22:41,200 --> 00:22:42,400
有很多问题

785
00:22:42,400 --> 00:22:43,333
那另外的话

786
00:22:43,333 --> 00:22:44,166
用MOE的好处

787
00:22:44,166 --> 00:22:45,766
就是当我们的用户数据量

788
00:22:45,766 --> 00:22:47,450
或者请求量比较大的时候

789
00:22:47,450 --> 00:22:49,083
推理的成本也会变高

790
00:22:49,083 --> 00:22:51,566
使用MOE会相对友好一点

791
00:22:51,966 --> 00:22:53,650
但不代表MOE的效果

792
00:22:53,650 --> 00:22:54,800
是很好的

793
00:22:55,166 --> 00:22:57,133
所以网络模型参数量变大了

794
00:22:57,133 --> 00:22:59,250
跟我们的效果是跟使不使用MOE

795
00:22:59,250 --> 00:23:01,366
它是没有直接正关联的关系

796
00:23:01,366 --> 00:23:02,600
因此我们现在看到

797
00:23:02,600 --> 00:23:04,333
大模型的结构的演进

798
00:23:04,333 --> 00:23:07,966
并不是从tranformer直接引进到MOE

799
00:23:07,966 --> 00:23:09,733
而是在两个之间

800
00:23:09,733 --> 00:23:10,683
他可以选择

801
00:23:10,683 --> 00:23:13,133
你可以选择自己的具体的方案

802
00:23:14,483 --> 00:23:15,283
那第二个

803
00:23:15,283 --> 00:23:17,283
就是面临数据枯竭的问题

804
00:23:17,283 --> 00:23:18,883
之前其实有很多人去问了

805
00:23:18,883 --> 00:23:19,400
这么老师

806
00:23:19,400 --> 00:23:21,000
未来数据会不会用完

807
00:23:21,000 --> 00:23:23,400
说实话在整个英伟达的Nemotron里面

808
00:23:23,400 --> 00:23:24,333
这个网络模型

809
00:23:24,333 --> 00:23:26,400
它提供了一个完全用合成的数据

810
00:23:26,400 --> 00:23:27,450
去训练之后

811
00:23:27,600 --> 00:23:29,533
现在从一个Llama3.1

812
00:23:29,533 --> 00:23:30,533
我们可以看到

813
00:23:30,533 --> 00:23:31,966
在整个Post-Training的阶段

814
00:23:31,966 --> 00:23:33,133
特别是SFT

815
00:23:33,166 --> 00:23:34,533
基本上已经产品化

816
00:23:34,533 --> 00:23:35,850
效果也是非常好

817
00:23:35,850 --> 00:23:37,250
通过合成的数据

818
00:23:37,483 --> 00:23:39,450
去对我们的网络模型进行微调

819
00:23:39,450 --> 00:23:41,733
而在整个之前谷歌Llama2里面

820
00:23:41,733 --> 00:23:43,683
也说了他用了合成的数据

821
00:23:43,683 --> 00:23:45,533
在SFT的一个具体阶段是

822
00:23:45,533 --> 00:23:46,400
可以看到

823
00:23:46,450 --> 00:23:47,083
国内

824
00:23:47,083 --> 00:23:48,366
不是国内

825
00:23:48,366 --> 00:23:49,533
国外巨头

826
00:23:49,533 --> 00:23:51,683
其实都已经开始慢慢的去用

827
00:23:51,683 --> 00:23:54,083
我们的一些合成数据

828
00:23:54,083 --> 00:23:56,133
而且通过实际验证证明

829
00:23:56,133 --> 00:23:57,333
合成数据质量

830
00:23:57,333 --> 00:23:58,600
其实并不比人工差

831
00:23:58,966 --> 00:23:59,250
也是

832
00:23:59,250 --> 00:24:01,133
你不要觉得大模型他是胡言乱语

833
00:24:01,133 --> 00:24:02,933
或者一本正经的胡言乱语

834
00:24:19,366 --> 00:24:21,850
他还是很有作用的

835
00:24:21,850 --> 00:24:24,166
最后一个就是大模型能力的上限了

836
00:24:24,166 --> 00:24:26,533
说实话大家觉得Llama3.1

837
00:24:26,800 --> 00:24:27,566
开源之后

838
00:24:27,566 --> 00:24:30,533
成为一个业界最强大的大模型

839
00:24:30,533 --> 00:24:31,650
或者最大

840
00:24:31,650 --> 00:24:34,133
405B的一个效果最好的大模型

841
00:24:34,133 --> 00:24:35,083
那大模型的能力

842
00:24:35,083 --> 00:24:36,800
或者传送门的能力上限在哪里

843
00:24:36,800 --> 00:24:38,600
还不能够继续的提升了

844
00:24:38,600 --> 00:24:39,933
说实话还是能够的

845
00:24:39,933 --> 00:24:41,850
同很多一些相关论文报道

846
00:24:41,850 --> 00:24:44,083
和相关的一些具体实践继续

847
00:24:44,083 --> 00:24:46,050
我们现在还可以去扩大网络模型

848
00:24:46,050 --> 00:24:47,283
和数据量的规模

849
00:24:47,283 --> 00:24:49,733
也就是继续的去遵循scaling law

850
00:24:49,933 --> 00:24:50,566
那第二种

851
00:24:50,566 --> 00:24:51,883
就是我们可能

852
00:24:51,883 --> 00:24:54,050
为了提升大模型的效果

853
00:24:54,050 --> 00:24:56,050
可能现在越来越的强调

854
00:24:56,050 --> 00:24:59,133
我们对于数据的质量的重要性了

855
00:24:59,133 --> 00:25:00,166
也就增长数据

856
00:25:00,166 --> 00:25:02,133
逻辑相关的一些代码

857
00:25:02,366 --> 00:25:03,566
最近大家很火的一点

858
00:25:03,566 --> 00:25:05,050
就是9.1

859
00:25:05,050 --> 00:25:06,250
是否大于9.9

860
00:25:06,250 --> 00:25:07,800
这个不断的去争论

861
00:25:07,800 --> 00:25:10,683
会给大模型一个理解

862
00:25:11,166 --> 00:25:12,566
这些都是很有意思的一点了

863
00:25:12,566 --> 00:25:13,283
今天的内容

864
00:25:13,283 --> 00:25:15,733
主要是跟大家具体的回顾了一下

865
00:25:15,766 --> 00:25:17,166
LLAMA3开源之后

866
00:25:17,166 --> 00:25:18,733
对具体技术分析

867
00:25:18,733 --> 00:25:19,733
从我们的数据

868
00:25:19,733 --> 00:25:20,400
模型结构

869
00:25:20,400 --> 00:25:20,850
预训练

870
00:25:20,850 --> 00:25:22,050
还有后处理

871
00:25:22,400 --> 00:25:24,400
做了一个真正洞察

872
00:25:24,400 --> 00:25:25,966
和技术方案分析

873
00:25:25,966 --> 00:25:26,283
然后

874
00:25:26,283 --> 00:25:28,600
对引起我们对业界的一个具体的思考

875
00:25:28,600 --> 00:25:29,450
那今天的内容

876
00:25:29,450 --> 00:25:30,283
就到这里为止了

877
00:25:30,283 --> 00:25:30,883
谢谢各位

878
00:25:30,883 --> 00:25:31,683
拜了个拜

