1
00:00:00,283 --> 00:00:02,283
内容/录制:Z0MI酱，视频剪辑/字幕:梁嘉铭

2
00:00:02,283 --> 00:00:03,766
哎呀录个课真不容易

3 
00:00:03,766 --> 00:00:05,450
ZOMI录这个大模型推理流程了

4
00:00:05,450 --> 00:00:06,933
已经录了快一个小时了

5
00:00:06,933 --> 00:00:08,083
然后NG了无数回

6
00:00:08,083 --> 00:00:09,966
然后现在又重新再来

7
00:00:17,650 --> 00:00:18,933
在整个大模型推理框架了

8
00:00:18,933 --> 00:00:21,200
我们现在来到了大模型推理流程

9
00:00:21,200 --> 00:00:22,000
这么一个阶段了

10
00:00:22,000 --> 00:00:22,766
今天重点

11
00:00:22,766 --> 00:00:25,050
去跟大家去分享Prefill跟Decoder阶段

12
00:00:25,050 --> 00:00:26,333
那这一个视频里面

13
00:00:26,333 --> 00:00:28,566
可能终于跟大家分开4个内容

14
00:00:28,566 --> 00:00:30,800
也就4个小节去做一个分享了

15
00:00:30,800 --> 00:00:31,400
那第一个内容

16
00:00:31,400 --> 00:00:32,933
就是大模型的推理的需求

17
00:00:32,933 --> 00:00:34,083
其实不断增长

18
00:00:34,083 --> 00:00:35,966
那面向大模型推理需求不断增长了

19
00:00:35,966 --> 00:00:38,450
我们要看一下整个大模型的推理过程

20
00:00:38,450 --> 00:00:39,133
到底是怎么样

21
00:00:39,133 --> 00:00:40,566
才能设计一个很好

22
00:00:40,566 --> 00:00:42,600
大模型的推理的引擎

23
00:00:42,600 --> 00:00:44,283
那在整个推理过程中

24
00:00:44,283 --> 00:00:46,566
实际上是分为Prefill跟decoder阶段

25
00:00:46,566 --> 00:00:48,800
为了把Prefill跟decoder阶段进行加速

26
00:00:48,800 --> 00:00:51,166
因此引入了KV Cache这个原理

27
00:00:51,166 --> 00:00:52,566
那有了KV Cache的原理了

28
00:00:52,566 --> 00:00:54,533
我们要看一下KV Cache的一个瓶颈

29
00:00:54,533 --> 00:00:55,366
进行分析

30
00:00:55,366 --> 00:00:56,450
最后倒推回来

31
00:00:56,450 --> 00:00:57,166
我们真正

32
00:00:57,166 --> 00:01:00,050
怎么去设计一个好的推理引擎

33
00:01:00,166 --> 00:01:00,483
所以

34
00:01:00,483 --> 00:01:02,883
我们今天来看一下简单的原理

35
00:01:02,883 --> 00:01:05,200
那是不是我们马上开始相关的内容

36
00:01:05,683 --> 00:01:06,566
第一个小节

37
00:01:06,566 --> 00:01:07,683
完全没有什么东西

38
00:01:07,683 --> 00:01:09,800
看一下大模型是一流的增长

39
00:01:09,800 --> 00:01:10,766
那现在来看

40
00:01:10,766 --> 00:01:11,800
不管是国内国外

41
00:01:11,800 --> 00:01:13,366
我们的闭源大模型也好

42
00:01:13,366 --> 00:01:14,166
开源大模型也好

43
00:01:14,166 --> 00:01:15,850
也是越来越多了

44
00:01:15,850 --> 00:01:16,683
那整体来看

45
00:01:16,683 --> 00:01:19,050
我们现在已经从传统的这种

46
00:01:19,050 --> 00:01:19,966
预训练大模型

47
00:01:19,966 --> 00:01:22,283
到了垂直领域的各种各样的大模型

48
00:01:22,483 --> 00:01:23,883
包括汽车上面教育大模型

49
00:01:23,883 --> 00:01:24,650
还有金融大模型

50
00:01:24,650 --> 00:01:26,250
各种各样的大模型都出现了

51
00:01:26,250 --> 00:01:28,366
除了我们的大模型很多以外

52
00:01:28,366 --> 00:01:29,483
我们现在的应用

53
00:01:29,483 --> 00:01:30,883
也是越来越多

54
00:01:30,883 --> 00:01:32,050
那这里面的每个LOGO

55
00:01:32,050 --> 00:01:33,250
对应的不同的应用

56
00:01:33,250 --> 00:01:35,800
可以看到大模型还是非常的有意思

57
00:01:35,850 --> 00:01:36,283
这里面

58
00:01:36,283 --> 00:01:38,650
ZOMI总结了三条的内容

59
00:01:38,650 --> 00:01:40,083
就大模型的推理的需求

60
00:01:40,083 --> 00:01:41,000
是不断的增长

61
00:01:41,000 --> 00:01:42,366
我们现在的大模型

62
00:01:42,366 --> 00:01:44,000
真的是推陈出新

63
00:01:44,000 --> 00:01:46,200
从Llama123到

64
00:01:46,200 --> 00:01:47,483
到现在的Llama3.1

65
00:01:47,483 --> 00:01:47,883
3.2

66
00:01:47,883 --> 00:01:48,450
3.3

67
00:01:48,450 --> 00:01:49,683
最近都出现了

68
00:01:49,883 --> 00:01:50,483
另外的话哈

69
00:01:50,483 --> 00:01:52,933
我们还有那个千问的12 2.5

70
00:01:52,933 --> 00:01:55,166
也是我们的大模型越来越多

71
00:01:55,166 --> 00:01:55,800
那另外的话

72
00:01:55,800 --> 00:01:57,483
我们大模型越来越多的同时

73
00:01:57,483 --> 00:01:59,450
同时我们的模型的参数量

74
00:01:59,450 --> 00:02:00,683
也是在增加

75
00:02:00,683 --> 00:02:02,133
那一开始的Llama从7B

76
00:02:02,133 --> 00:02:03,800
13B 35B到70B

77
00:02:03,800 --> 00:02:05,883
到现在Llama3的405B

78
00:02:06,083 --> 00:02:08,133
还有现在的good 300多B

79
00:02:08,133 --> 00:02:10,050
网络模型的规模的参数量

80
00:02:10,050 --> 00:02:11,250
是不断的增加

81
00:02:11,250 --> 00:02:11,800
另外的话

82
00:02:11,800 --> 00:02:13,933
我们看一下整个生成式的AI的应用

83
00:02:13,933 --> 00:02:15,483
也是爆发式的增长

84
00:02:15,483 --> 00:02:16,400
从去年开始

85
00:02:16,400 --> 00:02:18,450
我们看到一个大语言模型以外

86
00:02:18,450 --> 00:02:19,933
今年就是24年了

87
00:02:19,933 --> 00:02:22,133
可能你看视频的时候已经变成25年了

88
00:02:22,400 --> 00:02:23,333
在24年的时候

89
00:02:23,333 --> 00:02:26,400
已经出现了多模态的大模型图的大模型

90
00:02:26,400 --> 00:02:27,883
文生图视频的大模型

91
00:02:27,883 --> 00:02:29,966
各种各样的大模型也是出现

92
00:02:29,966 --> 00:02:32,283
所以我们整体的深层次的AI应用

93
00:02:32,600 --> 00:02:33,650
是越来越多

94
00:02:33,650 --> 00:02:35,883
随着我们的RAG的落地

95
00:02:35,883 --> 00:02:38,133
包括现在的强化微调的落地

96
00:02:38,133 --> 00:02:39,483
那我们整体来说

97
00:02:39,483 --> 00:02:40,933
整个大模型的推理的需求

98
00:02:40,933 --> 00:02:42,850
也是非常的旺盛

99
00:02:42,850 --> 00:02:44,333
那需求非常旺盛之后

100
00:02:44,333 --> 00:02:46,000
我们要真正的去看一下

101
00:02:46,000 --> 00:02:47,766
整个大模型的推理

102
00:02:47,766 --> 00:02:49,533
过程到底是怎么支撑好

103
00:02:49,533 --> 00:02:51,650
我们各种各样的模型的应用

104
00:02:51,650 --> 00:02:52,050
落地

105
00:02:52,050 --> 00:02:54,650
变成我们的推理引擎的核心的武器

106
00:02:54,966 --> 00:02:57,283
我们现在真的来到了第二个内容

107
00:02:57,283 --> 00:02:59,400
看一下LLM的一个推理过程了

108
00:02:59,400 --> 00:03:00,283
LLM的推理过程了

109
00:03:00,283 --> 00:03:03,250
我们首先一开始或者2023年时候

110
00:03:03,250 --> 00:03:04,933
其实大家对大语言模型的认知

111
00:03:04,933 --> 00:03:05,800
还是比较浅

112
00:03:05,966 --> 00:03:06,933
觉得大于模型

113
00:03:06,933 --> 00:03:08,600
我就丢一个Prompt的进去

114
00:03:08,600 --> 00:03:09,850
或者丢一个问题进去了

115
00:03:09,850 --> 00:03:12,283
然后他就输出一段话是吧

116
00:03:12,283 --> 00:03:12,883
但实际上

117
00:03:12,883 --> 00:03:14,600
在整个大模型运作的过程当中

118
00:03:14,600 --> 00:03:16,933
他输进去的是一段话

119
00:03:16,933 --> 00:03:17,366
但是

120
00:03:17,366 --> 00:03:20,450
我输出的是一个一个单词的输出

121
00:03:20,450 --> 00:03:21,883
每次选择概率最大

122
00:03:21,883 --> 00:03:23,600
或者选择我们的一个贪婪算法

123
00:03:23,600 --> 00:03:24,650
就我们的输入采样

124
00:03:24,650 --> 00:03:26,766
会讲怎么去选择这一个内容

125
00:03:26,766 --> 00:03:28,083
我们后面会讲到

126
00:03:28,083 --> 00:03:30,566
但是每次实际上只生成一个单词

127
00:03:30,566 --> 00:03:32,050
或者叫做一个词元

128
00:03:32,050 --> 00:03:33,966
一个Token的方式去做

129
00:03:34,083 --> 00:03:36,083
那这种我们叫做自回归的方式

130
00:03:36,083 --> 00:03:36,600
那具体

131
00:03:36,600 --> 00:03:37,800
我们打开这个图

132
00:03:37,800 --> 00:03:39,600
这个图就是网上非常有名的一个图

133
00:03:39,600 --> 00:03:42,133
我们输进去实际上是一个Prompt

134
00:03:42,133 --> 00:03:42,933
那每个Prompt

135
00:03:42,933 --> 00:03:45,133
是逐步的去输进去我们的整个大模型

136
00:03:45,133 --> 00:03:45,766
那大模型

137
00:03:45,766 --> 00:03:46,283
每一次

138
00:03:46,283 --> 00:03:48,566
都会输出一个单词或者一个词元

139
00:03:48,566 --> 00:03:50,050
用的是同一个大模型

140
00:03:50,050 --> 00:03:51,850
或者同一个Transformer堆叠的架构

141
00:03:51,850 --> 00:03:52,333
那这里面

142
00:03:52,333 --> 00:03:54,333
我们把它打开开看一下

143
00:03:54,333 --> 00:03:56,483
首先我们有一个Prompt who are you

144
00:03:56,483 --> 00:03:56,883
然后

145
00:03:56,883 --> 00:03:58,933
同时丢给我们的整个Transformer的架构

146
00:03:58,933 --> 00:03:59,800
那Transformer架构

147
00:03:59,800 --> 00:04:01,683
第一次会输出一个I am

148
00:04:01,683 --> 00:04:03,283
也就是对应的这里面的一个单词

149
00:04:03,283 --> 00:04:04,650
我们的第一个Token

150
00:04:04,650 --> 00:04:05,250
那接着

151
00:04:05,250 --> 00:04:07,883
我们会把这个全部都拼起来

152
00:04:07,883 --> 00:04:10,333
重新的丢给我们的大模型

153
00:04:10,333 --> 00:04:11,366
然后这个大模型

154
00:04:11,366 --> 00:04:14,050
就会生出输出第二个Token

155
00:04:14,050 --> 00:04:15,533
也就是第二个词元

156
00:04:15,650 --> 00:04:17,166
同样的我们会把这个单词

157
00:04:17,166 --> 00:04:18,483
全部都拼起来之后

158
00:04:18,483 --> 00:04:20,966
第三次丢给我们的大模型

159
00:04:20,966 --> 00:04:22,850
当然了这个模型都是同一个模型

160
00:04:22,850 --> 00:04:24,366
而不是3个不同的模型

161
00:04:24,366 --> 00:04:25,050
然后

162
00:04:25,050 --> 00:04:28,166
再生成下一个单词或下一个Token

163
00:04:28,200 --> 00:04:29,050
这种方式

164
00:04:29,050 --> 00:04:30,683
因此我们叫做自回归

165
00:04:30,683 --> 00:04:32,000
auto regress的方式

166
00:04:32,000 --> 00:04:34,883
去实现我们整个大模型的推理

167
00:04:35,050 --> 00:04:36,733
在整个大模型推理的过程当中

168
00:04:36,733 --> 00:04:37,650
我们会发现

169
00:04:37,650 --> 00:04:39,650
其实刚才我们所有的单词

170
00:04:39,650 --> 00:04:41,533
或者我们已经准备好了一个Prompt

171
00:04:41,566 --> 00:04:43,050
那这个Prompt的过程当中

172
00:04:43,050 --> 00:04:44,366
我们叫做预填充

173
00:04:44,366 --> 00:04:46,333
也就英文叫做prefill的阶段

174
00:04:46,333 --> 00:04:47,766
那prefill阶段之后

175
00:04:47,766 --> 00:04:50,133
会真正的输出第一个Token

176
00:04:50,133 --> 00:04:51,400
那有了第一个Token之后

177
00:04:51,400 --> 00:04:52,333
我们会把后面

178
00:04:52,333 --> 00:04:53,766
或者把前面的所有的Token

179
00:04:53,766 --> 00:04:54,566
都拼接起来

180
00:04:54,566 --> 00:04:56,683
丢第二次丢给我们的一个大模型

181
00:04:56,683 --> 00:04:57,850
那这个过程当中

182
00:04:57,850 --> 00:04:58,883
就自回归的过程

183
00:04:58,883 --> 00:05:00,400
我们叫做decording

184
00:05:00,450 --> 00:05:01,650
解码的过程

185
00:05:01,650 --> 00:05:04,000
每次解码一个单词或者一个词元

186
00:05:04,800 --> 00:05:06,133
因此我们这里面

187
00:05:06,133 --> 00:05:07,683
在整个大语模型里面

188
00:05:07,683 --> 00:05:09,083
就分开两个过程了

189
00:05:09,083 --> 00:05:10,083
一个叫做Prefill

190
00:05:10,083 --> 00:05:11,450
一个叫做decord

191
00:05:11,450 --> 00:05:12,366
那Prefill阶段了

192
00:05:12,366 --> 00:05:14,600
我们还是要学术性的定义一下

193
00:05:14,600 --> 00:05:16,800
大家看一下简单的下面里面的内容了

194
00:05:16,800 --> 00:05:18,450
我们根据输入的Token

195
00:05:18,450 --> 00:05:21,883
输出第一个输出的单词或第一个Token

196
00:05:21,883 --> 00:05:24,650
那完成一个Transformer的词前向的过程

197
00:05:24,650 --> 00:05:26,283
那Transformer的词前向过程当中

198
00:05:26,283 --> 00:05:27,533
我们一开始输入

199
00:05:27,533 --> 00:05:28,733
的所有Token

200
00:05:28,733 --> 00:05:30,283
它可以做并形的执行

201
00:05:30,283 --> 00:05:31,933
因此执行的效率很高

202
00:05:31,933 --> 00:05:34,250
这里面的用到的也是一个complete Bound

203
00:05:34,250 --> 00:05:36,650
也是我们大量的需要算力

204
00:05:36,883 --> 00:05:38,366
那在第二个过程当中

205
00:05:38,366 --> 00:05:40,483
我们生成了第一个Token之后

206
00:05:40,483 --> 00:05:41,766
采用regress的方式

207
00:05:41,766 --> 00:05:42,966
也就自回归的方式

208
00:05:42,966 --> 00:05:44,933
一次只生成一个Token

209
00:05:44,933 --> 00:05:45,766
那这个时候

210
00:05:45,766 --> 00:05:47,850
直到什么时候才会结束

211
00:05:47,850 --> 00:05:51,450
直到我们的标记符出现才会去结束

212
00:05:51,450 --> 00:05:53,333
那假设我们一共要输出

213
00:05:53,333 --> 00:05:56,083
也就我们一共要回答很多一段话

214
00:05:56,650 --> 00:05:58,200
也就是我们对应回答一段话

215
00:05:58,200 --> 00:05:59,733
里面有很多单词

216
00:05:59,733 --> 00:06:03,050
假设我们给大模型说你编一个故事吧

217
00:06:03,050 --> 00:06:05,766
给我然后我需要的次数是1万字

218
00:06:05,766 --> 00:06:06,450
那这个时候

219
00:06:06,450 --> 00:06:07,283
我们的n

220
00:06:07,283 --> 00:06:09,083
输出的n就会非常的多

221
00:06:09,083 --> 00:06:09,933
decode阶段

222
00:06:09,933 --> 00:06:12,600
就执行n减一次的词前向

223
00:06:12,600 --> 00:06:13,933
那这n减一次的词前向

224
00:06:13,933 --> 00:06:15,650
只能的去串行执行

225
00:06:15,650 --> 00:06:17,883
所以它增减的效率是非常的低

226
00:06:17,883 --> 00:06:19,533
那在生成的过程当中

227
00:06:19,533 --> 00:06:21,683
我们需要关注的Token越来越多

228
00:06:21,683 --> 00:06:22,766
整体的计算量

229
00:06:22,766 --> 00:06:24,250
也会去增加

230
00:06:24,250 --> 00:06:27,000
所以既然计算量在增加

231
00:06:27,000 --> 00:06:28,966
但是我们只能串行执行

232
00:06:28,966 --> 00:06:31,650
于是在整个大模型推理过程当中

233
00:06:31,650 --> 00:06:32,600
decode阶段

234
00:06:32,600 --> 00:06:34,333
就会遇到瓶颈了

235
00:06:35,133 --> 00:06:38,083
为了解决刚才decode阶段又要串行

236
00:06:38,083 --> 00:06:39,200
计算力又不断增大

237
00:06:39,200 --> 00:06:40,083
于是业界

238
00:06:40,083 --> 00:06:41,133
后来我们的科学家

239
00:06:41,133 --> 00:06:44,050
又发明了一个新的算法叫做KV cache

240
00:06:44,050 --> 00:06:46,050
那接下来我们就看一下KV cache

241
00:06:46,050 --> 00:06:48,650
怎么去解决decode的阶段

242
00:06:48,683 --> 00:06:49,733
计算量越来越大

243
00:06:49,733 --> 00:06:52,850
然后只能串行这么一个相关的困难

244
00:06:52,883 --> 00:06:55,000
那在整个KV cache的过程当中

245
00:06:55,000 --> 00:06:57,333
我们看一下还是这个图

246
00:06:57,333 --> 00:06:59,133
首先我们会有一个prefill的阶段

247
00:06:59,133 --> 00:07:01,483
我们有T0到T2个Token

248
00:07:01,683 --> 00:07:03,400
然后我们会做一个自解码

249
00:07:03,400 --> 00:07:04,650
也就是不断的decode

250
00:07:04,650 --> 00:07:06,250
不断的decode的过程当中

251
00:07:06,250 --> 00:07:07,883
那这里面一共有6个Token

252
00:07:07,883 --> 00:07:09,450
我们会在一开始的时候

253
00:07:09,450 --> 00:07:09,966
Prefill阶段

254
00:07:09,966 --> 00:07:11,133
预填充的阶段

255
00:07:11,133 --> 00:07:12,650
去把T0 T1

256
00:07:12,650 --> 00:07:14,566
T2三个Token的KV cache

257
00:07:14,566 --> 00:07:16,850
跟KV cache缓存起来

258
00:07:16,850 --> 00:07:18,166
那两个cash缓存起来

259
00:07:18,166 --> 00:07:20,000
我们统称KV cache

260
00:07:20,000 --> 00:07:22,083
所以它实际上是两个缓存

261
00:07:22,083 --> 00:07:23,083
而不是一个缓存

262
00:07:23,083 --> 00:07:24,733
把它统称起来而已

263
00:07:24,733 --> 00:07:26,366
那等我们执行T3的时候

264
00:07:26,366 --> 00:07:28,250
我会把T3的一个KVCache开始

265
00:07:28,250 --> 00:07:29,483
同样的缓存起来

266
00:07:29,483 --> 00:07:30,250
拼接起来

267
00:07:30,250 --> 00:07:32,366
然后给到第四个KVCache开始的时候

268
00:07:32,366 --> 00:07:33,366
去计算

269
00:07:33,366 --> 00:07:34,800
同样的以此类推

270
00:07:34,800 --> 00:07:36,250
我们每生成一个Tok

271
00:07:36,250 --> 00:07:38,600
我们都会把之前中间的过程

272
00:07:38,600 --> 00:07:40,250
KV cache缓存起来

273
00:07:40,250 --> 00:07:42,766
通过KV cache的缓存去解决

274
00:07:42,766 --> 00:07:43,883
我们刚才讲到

275
00:07:43,883 --> 00:07:44,483
每一次

276
00:07:44,483 --> 00:07:46,166
都要把前面的所有的句子

277
00:07:46,166 --> 00:07:47,166
都拼接起来

278
00:07:47,166 --> 00:07:49,650
重新丢给我们的一个大模型

279
00:07:49,683 --> 00:07:51,400
那下面我们还是学术点

280
00:07:51,400 --> 00:07:53,850
或者我们简单的看一下这个流程

281
00:07:53,850 --> 00:07:54,333
每一次

282
00:07:54,333 --> 00:07:55,450
我们都会把Token

283
00:07:55,450 --> 00:07:56,400
经过Transformer的时候

284
00:07:56,400 --> 00:07:59,883
乘以wk乘以wv的两个矩阵结果缓存起来

285
00:07:59,883 --> 00:08:00,966
说我们刚才讲到了

286
00:08:00,966 --> 00:08:04,450
他k跟v是两个不同的一个缓存

287
00:08:04,450 --> 00:08:05,133
训练时候

288
00:08:05,133 --> 00:08:05,966
他是不需要

289
00:08:05,966 --> 00:08:06,933
大家值得注意了

290
00:08:06,933 --> 00:08:08,966
我们只有在推理的时候去需要

291
00:08:09,650 --> 00:08:11,283
为什么训练的时候不需要

292
00:08:11,566 --> 00:08:12,883
这个我们后面会讲

293
00:08:12,883 --> 00:08:14,400
然后在推理解码的时候

294
00:08:14,400 --> 00:08:15,483
因为我们刚才讲到了

295
00:08:15,483 --> 00:08:17,600
它采用了一个auto-regressive

296
00:08:17,600 --> 00:08:18,850
也就是自编码的方式

297
00:08:18,850 --> 00:08:20,966
所以我们每次只能生成一个Token

298
00:08:20,966 --> 00:08:22,400
它只能串性的执行

299
00:08:22,400 --> 00:08:23,000
每一次

300
00:08:23,000 --> 00:08:23,766
都要依赖于

301
00:08:23,766 --> 00:08:26,166
之前的一个Token的一个结果

302
00:08:26,166 --> 00:08:28,400
因此我们每一次生成Token的时候

303
00:08:28,400 --> 00:08:30,766
都需要从一个Wk跟Wv

304
00:08:30,933 --> 00:08:32,850
然后每一次都对所有的Token进行编译

305
00:08:32,850 --> 00:08:33,966
代价就非常大

306
00:08:33,966 --> 00:08:35,800
所以我们把它缓存起来了

307
00:08:35,800 --> 00:08:36,250
那这个

308
00:08:36,250 --> 00:08:38,050
就是我们刚才讲到的一个原理

309
00:08:38,050 --> 00:08:38,566
那下面

310
00:08:38,566 --> 00:08:41,133
我们其实还有一个example

311
00:08:41,133 --> 00:08:42,766
the largest city of China is

312
00:08:42,800 --> 00:08:44,366
这里面大家自己看一下就好了

313
00:08:44,366 --> 00:08:47,000
ZOMI就不在这里面详细的论述了

314
00:08:48,850 --> 00:08:49,933
好我们继续过

315
00:08:49,933 --> 00:08:51,283
有兴趣的小伙伴也可以停下来

316
00:08:51,283 --> 00:08:52,333
自己去看

317
00:08:52,933 --> 00:08:54,083
ZOMI老师你好

318
00:08:54,083 --> 00:08:55,366
我有个问题

319
00:08:55,650 --> 00:08:57,333
为什么只有KV cache

320
00:08:57,333 --> 00:08:59,000
没有Q cache

321
00:08:59,933 --> 00:09:03,083
嗯小新的问题问的非常好

322
00:09:03,083 --> 00:09:04,083
在训练的过程当中

323
00:09:04,083 --> 00:09:05,600
我们都没有cache

324
00:09:05,600 --> 00:09:07,283
因为每一次都是新增

325
00:09:07,283 --> 00:09:08,800
或者每一次大量的计算

326
00:09:08,800 --> 00:09:10,483
那为什么会有KV cache

327
00:09:10,483 --> 00:09:11,400
我们还是要看一下

328
00:09:11,400 --> 00:09:12,800
整个attention机制里面

329
00:09:12,800 --> 00:09:14,166
最核心的计算

330
00:09:14,166 --> 00:09:14,766
那这里面

331
00:09:14,766 --> 00:09:16,083
我们还是有一个图

332
00:09:16,083 --> 00:09:18,283
那在整个Transformer架构里面

333
00:09:18,283 --> 00:09:20,483
最核心的就自注意力机制嘛

334
00:09:20,483 --> 00:09:21,966
那自注意力机制里面

335
00:09:21,966 --> 00:09:23,600
就是我们右边的这个图了

336
00:09:23,600 --> 00:09:25,200
有QKV三个矩形

337
00:09:25,200 --> 00:09:27,300
经过一个Scaled Dot-Product attention

338
00:09:27,300 --> 00:09:28,333
这计算之后

339
00:09:28,450 --> 00:09:30,333
把它三个得到的结果

340
00:09:30,333 --> 00:09:31,100
concat起来

341
00:09:31,100 --> 00:09:33,016
然后再给xxxx去计算

342
00:09:33,016 --> 00:09:35,500
那真正去计算Scaled Dot-Produet Attention

343
00:09:35,500 --> 00:09:36,616
就是左边

344
00:09:36,616 --> 00:09:37,533
的这个图

345
00:09:37,533 --> 00:09:39,416
那左边的q跟k进行相乘

346
00:09:39,416 --> 00:09:40,500
要经过一个归一化

347
00:09:40,500 --> 00:09:41,850
然后Mask出来之后

348
00:09:41,850 --> 00:09:42,966
经过一个Softmax

349
00:09:42,966 --> 00:09:44,450
然后Softmax的结果

350
00:09:44,450 --> 00:09:46,566
跟我们的v进行相乘

351
00:09:46,566 --> 00:09:47,900
当然了在训练的过程当中

352
00:09:47,900 --> 00:09:49,366
有Mask在推理的过程当中

353
00:09:49,366 --> 00:09:50,850
是没有Mask的

354
00:09:51,250 --> 00:09:53,050
那我们看一下刚才的一个公式

355
00:09:53,050 --> 00:09:53,500
实际上

356
00:09:53,500 --> 00:09:56,050
就变成上面的这一条公式了

357
00:09:56,050 --> 00:09:57,766
我们的q乘以k

358
00:09:57,766 --> 00:09:58,133
然后

359
00:09:58,133 --> 00:10:00,850
经过一个scale的或者归一化的过程当中

360
00:10:00,850 --> 00:10:02,933
然后给我们的softmax进行计算

361
00:10:02,933 --> 00:10:04,533
然后乘以我们的v

362
00:10:04,566 --> 00:10:09,333
那这个就是我们真正的multi head attention的一个最核心的计算

363
00:10:09,900 --> 00:10:11,166
那这里面这个问题

364
00:10:11,166 --> 00:10:12,850
嗯因为刚才的例子没有讲

365
00:10:12,850 --> 00:10:14,050
所以这个问题有点错误

366
00:10:14,050 --> 00:10:14,733
不过没关系

367
00:10:14,733 --> 00:10:15,850
我们还是看一下

368
00:10:15,850 --> 00:10:18,450
ZOMI希望尽可能的给大家讲明白

369
00:10:18,450 --> 00:10:19,966
那假设我们现在Prompt

370
00:10:19,966 --> 00:10:21,766
有the large city of China is

371
00:10:21,900 --> 00:10:24,766
我们需要预测或者生成下一个Token

372
00:10:24,766 --> 00:10:26,016
下Token是上

373
00:10:26,016 --> 00:10:26,933
那这个时候

374
00:10:26,933 --> 00:10:28,566
上了我们需要有一个q

375
00:10:28,566 --> 00:10:31,016
我们真正的the large city of China is

376
00:10:31,016 --> 00:10:33,250
我们会把它缓存起来KV

377
00:10:33,250 --> 00:10:34,300
然后跟这个q

378
00:10:34,300 --> 00:10:36,766
进行一个重新的结合去计算

379
00:10:36,766 --> 00:10:39,166
因此我们在真正的一个计算过程中

380
00:10:39,166 --> 00:10:40,566
主要是依赖于两个内容

381
00:10:40,566 --> 00:10:41,016
第一个

382
00:10:41,016 --> 00:10:42,816
我们依赖于q的一行

383
00:10:42,850 --> 00:10:45,616
另外的话我们依赖于k跟v的一个cache

384
00:10:45,650 --> 00:10:46,166
那这里面

385
00:10:46,166 --> 00:10:48,816
我们在以训练的场景为例子

386
00:10:48,816 --> 00:10:50,450
因为我们还是要真正的看一下

387
00:10:50,450 --> 00:10:51,816
在整个训练过程当中

388
00:10:51,816 --> 00:10:53,966
跟推理过程当中

389
00:10:54,050 --> 00:10:56,616
整个KV cache在里面

390
00:10:56,616 --> 00:11:00,133
或者KQV三个是怎么去相乘

391
00:11:00,133 --> 00:11:01,300
那在训练过程当中

392
00:11:01,300 --> 00:11:02,533
我们可以看到这里面

393
00:11:02,533 --> 00:11:04,300
我们就以一个例子

394
00:11:04,416 --> 00:11:05,766
里面我们有三个单词

395
00:11:05,766 --> 00:11:06,416
有框

396
00:11:06,416 --> 00:11:07,766
还有machine

397
00:11:07,850 --> 00:11:10,166
然后Quant machine is什么

398
00:11:10,166 --> 00:11:12,133
我们要预测下个单词的时候

399
00:11:12,133 --> 00:11:14,816
实际上我们预测的是蓝色的这个框框

400
00:11:14,816 --> 00:11:15,933
蓝色的这框框了

401
00:11:15,933 --> 00:11:18,100
现在是被mask的掉

402
00:11:18,100 --> 00:11:19,133
那mask掉之后

403
00:11:19,133 --> 00:11:22,216
我们再真正因为mask没关系

404
00:11:22,216 --> 00:11:23,650
那我们QKV mask之后

405
00:11:23,650 --> 00:11:24,566
其实矩阵大小

406
00:11:24,566 --> 00:11:26,250
跟我们的v的矩阵大小是一样

407
00:11:26,250 --> 00:11:27,616
我们再进行一个相乘

408
00:11:27,616 --> 00:11:29,766
然后得到我们下一个单词

409
00:11:29,766 --> 00:11:30,700
这么一个内容

410
00:11:30,700 --> 00:11:32,933
当然我们这里面是一行

411
00:11:33,100 --> 00:11:33,700
这里面的一行

412
00:11:33,700 --> 00:11:38,566
从S30 31 32 33乘以这里面的每一列

413
00:11:38,566 --> 00:11:39,733
v里面的每一列

414
00:11:39,733 --> 00:11:42,766
我们才能够得到后面的这么一行

415
00:11:42,766 --> 00:11:44,333
也就QKV的一行

416
00:11:44,333 --> 00:11:44,850
所以这里面

417
00:11:44,850 --> 00:11:45,900
大家有没有发现

418
00:11:45,900 --> 00:11:49,566
我这里面真正的q是用到了一行

419
00:11:49,566 --> 00:11:51,650
但是我的v或者我的k

420
00:11:51,650 --> 00:11:53,700
是用到整个大的矩阵

421
00:11:53,700 --> 00:11:56,016
说我们的q是重新的生成

422
00:11:56,016 --> 00:11:56,766
然后k v

423
00:11:56,766 --> 00:11:58,933
它可以大量的缓存起来

424
00:11:58,933 --> 00:11:59,416
那现在

425
00:11:59,416 --> 00:12:01,366
我们看一下具体的一个推理的过程

426
00:12:01,366 --> 00:12:02,133
当中的例子

427
00:12:02,133 --> 00:12:03,816
刚才只是做一个训练

428
00:12:03,816 --> 00:12:04,700
推理的过程中

429
00:12:04,700 --> 00:12:07,133
我们是预测下一个单词

430
00:12:07,133 --> 00:12:09,700
那我们现在有前面两个单词

431
00:12:09,700 --> 00:12:12,016
然后我们在第三个单词的时候

432
00:12:12,016 --> 00:12:14,300
其实是预测第三个单词

433
00:12:14,300 --> 00:12:16,566
Quantum machine is什么

434
00:12:16,566 --> 00:12:19,016
那我们当时候当前的单词输进去

435
00:12:19,016 --> 00:12:19,933
是一个machine

436
00:12:20,100 --> 00:12:22,566
然后对应的k跟v的缓存

437
00:12:22,566 --> 00:12:25,816
是前面三个单词都缓存起来

438
00:12:25,816 --> 00:12:27,816
然后通过这种方式去计算

439
00:12:27,816 --> 00:12:31,300
我们把q的一行乘以k的每一列

440
00:12:31,300 --> 00:12:34,100
得到三面的三个小框框

441
00:12:34,133 --> 00:12:35,933
同样的我们会把q乘以k

442
00:12:35,933 --> 00:12:37,816
Softmax之后的一行

443
00:12:37,933 --> 00:12:40,616
乘以我们的v里面的每一列

444
00:12:40,616 --> 00:12:41,966
然后才能够得到

445
00:12:41,966 --> 00:12:44,700
我们最终的输出的QKV的一行

446
00:12:44,700 --> 00:12:47,533
因此我们在真正的推理过程当中

447
00:12:47,533 --> 00:12:48,733
为了得到下一个Token

448
00:12:48,733 --> 00:12:50,616
实际上我是使用当前的话

449
00:12:50,616 --> 00:12:51,700
上一个Token

450
00:12:51,700 --> 00:12:54,500
然后还有之前的所有的k跟v

451
00:12:54,500 --> 00:12:55,566
那大家看这个图

452
00:12:55,566 --> 00:12:57,100
应该非常的清楚

453
00:12:57,100 --> 00:12:59,133
我们每生成的一个Tl'ken

454
00:12:59,133 --> 00:13:01,533
需要的是上一个Token的q

455
00:13:01,566 --> 00:13:05,850
还有之前Token的所有的k跟v的缓存

456
00:13:05,850 --> 00:13:07,816
所以我们需要两个内容

457
00:13:10,050 --> 00:13:11,250
接着我们还是要看一下

458
00:13:11,250 --> 00:13:13,166
在整个KV cache里面

459
00:13:13,166 --> 00:13:15,016
把我们的prefill跟decode阶段

460
00:13:15,016 --> 00:13:15,816
融合起来

461
00:13:15,816 --> 00:13:16,300
那下面

462
00:13:16,300 --> 00:13:18,533
我们把整个Prompt的过程当中

463
00:13:18,566 --> 00:13:20,733
丢进去我们的大模型里面

464
00:13:20,733 --> 00:13:22,616
进行一个前向的计算

465
00:13:22,616 --> 00:13:24,500
然后因为采用KV cache的原理

466
00:13:24,500 --> 00:13:28,166
我们就把之前的一个T0 T1 T2缓存起来

467
00:13:28,166 --> 00:13:30,100
那真正做下一个单词预测的时候

468
00:13:30,100 --> 00:13:33,050
我们就会把T0 T1和T2的kv cache

469
00:13:33,050 --> 00:13:33,566
缓存起来

470
00:13:33,566 --> 00:13:35,733
然后得到我们的T3的一个KV cache

471
00:13:35,733 --> 00:13:36,733
也缓存起来

472
00:13:36,733 --> 00:13:38,616
真正做第四个Token计算的时候

473
00:13:38,616 --> 00:13:40,216
我们才会真正的去计算

474
00:13:40,216 --> 00:13:41,933
然后计算完之后又缓存起来

475
00:13:41,933 --> 00:13:43,450
不断的把我们的KV cache

476
00:13:43,450 --> 00:13:44,500
进行一个拼接

477
00:13:44,500 --> 00:13:46,166
但是大家有没有发现

478
00:13:46,166 --> 00:13:48,300
这里面还是有个问题

479
00:13:50,766 --> 00:13:52,016
也就是第一个问题

480
00:13:52,016 --> 00:13:53,733
就是随着我们的Prompt的数量

481
00:13:53,733 --> 00:13:54,900
变得越来越长

482
00:13:54,900 --> 00:13:55,933
越来越多的时候

483
00:13:55,933 --> 00:13:59,333
我们整体缓存的KV cache也会越来越大

484
00:13:59,333 --> 00:13:59,966
那这个时候

485
00:13:59,966 --> 00:14:02,300
会对我们的显存造成很大的压力

486
00:14:02,533 --> 00:14:03,100
那第二个

487
00:14:03,100 --> 00:14:05,250
就是我们的输出的序列

488
00:14:05,250 --> 00:14:06,900
是没有办法去预测

489
00:14:06,900 --> 00:14:07,566
所以我们很难

490
00:14:07,566 --> 00:14:09,416
前提为我们整个q开始

491
00:14:09,416 --> 00:14:12,500
去量身定制我们的存储空间或内存空间

492
00:14:12,500 --> 00:14:13,100
那这个时候

493
00:14:13,100 --> 00:14:14,900
就会导致我们的缓存

494
00:14:14,966 --> 00:14:16,300
或者我们的显存

495
00:14:16,300 --> 00:14:17,733
造成极大的浪费

496
00:14:17,733 --> 00:14:19,500
和不断的开辟我们的空间了

497
00:14:19,500 --> 00:14:21,333
那这个我们在学习操作系统的时候

498
00:14:21,333 --> 00:14:23,733
其实嗯会有相应的课程

499
00:14:23,733 --> 00:14:27,450
就是我们的显存会有大量的碎片页

500
00:14:27,450 --> 00:14:29,816
于是后面为了解决KV KV cache的问题

501
00:14:29,816 --> 00:14:32,766
又提出了各种各样的新的算法

502
00:14:33,566 --> 00:14:33,933
这里面

503
00:14:33,933 --> 00:14:37,816
ZOMI讲的还是相对比较简洁一点

504
00:14:37,816 --> 00:14:39,050
如果有兴趣的小伙伴

505
00:14:39,050 --> 00:14:40,366
也可以打开这个PPT

506
00:14:40,366 --> 00:14:43,100
好好的去了解一下里面的一些原理

507
00:14:43,100 --> 00:14:45,533
或者看一下里面的一些摘要

508
00:14:45,533 --> 00:14:46,133
那这里面

509
00:14:46,133 --> 00:14:48,566
我们还是回到整个KV cache里面

510
00:14:48,566 --> 00:14:50,650
因为为了做好一个AI框架

511
00:14:50,650 --> 00:14:52,166
或者做好一个推理引擎

512
00:14:52,166 --> 00:14:54,450
我们还是要对KV cache的瓶颈

513
00:14:54,450 --> 00:14:56,050
进行分析

514
00:14:56,166 --> 00:14:57,900
那我们做瓶颈分析的时候

515
00:14:57,900 --> 00:14:58,500
你们会发现

516
00:14:58,500 --> 00:14:59,816
在训练的过程当中

517
00:14:59,816 --> 00:15:02,450
因为我们不需要把q跟k

518
00:15:02,450 --> 00:15:03,500
缓存起来

519
00:15:03,500 --> 00:15:04,300
所以我们

520
00:15:04,300 --> 00:15:06,700
需要有大量的矩阵的计算

521
00:15:06,700 --> 00:15:08,100
从我们一开始的embedding 

522
00:15:08,100 --> 00:15:09,333
把输进去一个vector

523
00:15:09,450 --> 00:15:10,016
然后乘

524
00:15:10,016 --> 00:15:12,333
以QKV然后因为有Batch size嘛

525
00:15:12,333 --> 00:15:13,216
所以我们这个矩阵

526
00:15:13,216 --> 00:15:14,616
就变得非常的大

527
00:15:14,616 --> 00:15:15,850
那这个很大的矩阵

528
00:15:15,850 --> 00:15:17,300
因为要进行一个mask

529
00:15:17,300 --> 00:15:18,616
mask完了再乘以v

530
00:15:18,650 --> 00:15:19,700
所以大部分情况下

531
00:15:19,700 --> 00:15:23,700
它是一个计算密集型的一个情况

532
00:15:23,733 --> 00:15:25,450
那有了计算密集型

533
00:15:25,450 --> 00:15:27,566
我们同样的会有内存密集型

534
00:15:27,566 --> 00:15:29,966
也就是computer Bound memory Bound相关的内容

535
00:15:30,133 --> 00:15:32,250
因为在整个大语言模型里面

536
00:15:32,250 --> 00:15:33,016
我们的推理

537
00:15:33,016 --> 00:15:34,533
实际是分开两个阶段

538
00:15:34,533 --> 00:15:35,966
一个是Prefill的阶段

539
00:15:35,966 --> 00:15:37,333
一个是decord的阶段

540
00:15:37,333 --> 00:15:38,733
那Prefill的阶段

541
00:15:38,733 --> 00:15:39,133
实际上

542
00:15:39,133 --> 00:15:40,900
它是一个计算密集型

543
00:15:40,900 --> 00:15:42,816
我们现在来假设我们的服务请求了

544
00:15:42,816 --> 00:15:43,850
Batch size只有一

545
00:15:43,850 --> 00:15:45,566
也就只有一个服务的请求

546
00:15:45,616 --> 00:15:48,333
随着我们的sequence lens越来越长了

547
00:15:48,733 --> 00:15:51,050
也就是我们丢进去的prompt越来越长

548
00:15:51,050 --> 00:15:53,500
我们整体的一个算力的峰值或吞吐

549
00:15:53,500 --> 00:15:54,733
其实是

550
00:15:54,733 --> 00:15:55,450
会停滞

551
00:15:55,450 --> 00:15:57,050
因为我们对算力的需求

552
00:15:57,050 --> 00:15:58,500
是不断的增加

553
00:15:58,500 --> 00:16:00,566
所以我们的三角形

554
00:16:00,566 --> 00:16:02,133
就是我们Prefill阶段

555
00:16:02,133 --> 00:16:03,933
我们会遇到一个computer棒

556
00:16:03,933 --> 00:16:04,616
那另外的话

557
00:16:04,616 --> 00:16:06,500
我们看一下圆形这个框框

558
00:16:06,500 --> 00:16:07,366
圆形这框框

559
00:16:07,366 --> 00:16:08,766
随着我们的size越大

560
00:16:08,766 --> 00:16:10,533
我们的计算强度也会越大

561
00:16:10,650 --> 00:16:12,250
然后我们对于内存的消耗

562
00:16:12,250 --> 00:16:13,300
也会越来越大

563
00:16:13,300 --> 00:16:15,216
但是我们一开始的时候

564
00:16:15,216 --> 00:16:17,616
没有办法完全去消耗完我们的算力

565
00:16:17,616 --> 00:16:18,250
其实更多

566
00:16:18,250 --> 00:16:20,100
是对我们的内存的一个消耗

567
00:16:20,100 --> 00:16:20,850
所以我们的这里面

568
00:16:20,850 --> 00:16:22,133
是属于memory Bound

569
00:16:22,133 --> 00:16:23,933
那分析完commune半跟memory Bound了

570
00:16:23,933 --> 00:16:25,250
我们就很清楚的知道

571
00:16:25,250 --> 00:16:27,016
我们在profill的阶段

572
00:16:27,133 --> 00:16:29,700
需要的是算力decode阶

573
00:16:29,700 --> 00:16:31,733
段我们需要的是显存

574
00:16:31,733 --> 00:16:33,816
但是算力多大为合适

575
00:16:33,816 --> 00:16:35,250
显存多大为合适

576
00:16:35,250 --> 00:16:37,533
我们可能需要打开具体的模型

577
00:16:37,533 --> 00:16:38,333
作为例子

578
00:16:38,333 --> 00:16:38,966
那这里面

579
00:16:38,966 --> 00:16:39,566
有一篇论文

580
00:16:39,566 --> 00:16:42,166
就讲了他在A6000的一个GPU里面

581
00:16:42,166 --> 00:16:43,816
训练的一个Llama

582
00:16:44,100 --> 00:16:46,500
应该是推理一个Llama的13B的模型

583
00:16:46,500 --> 00:16:47,050
那这里面

584
00:16:47,050 --> 00:16:48,766
我们随着Batch size越大

585
00:16:48,766 --> 00:16:50,500
可以看到整体的perfill阶段

586
00:16:50,500 --> 00:16:51,500
我们的吞吐

587
00:16:51,500 --> 00:16:53,966
其实是受到我们的算力所约束

588
00:16:53,966 --> 00:16:55,766
所以后面就没有再增长了

589
00:16:55,766 --> 00:16:57,850
但是我们在decode的阶段

590
00:16:57,850 --> 00:17:00,133
因为是自编码或者自回归的方式

591
00:17:00,133 --> 00:17:01,500
我们在Batch size越大

592
00:17:01,500 --> 00:17:02,966
然后我们的sequence names越大

593
00:17:02,966 --> 00:17:04,216
我们整体的吞吐

594
00:17:04,216 --> 00:17:06,416
是没有完全的像刚才那样

595
00:17:06,416 --> 00:17:07,933
左边的这个图打满

596
00:17:07,933 --> 00:17:08,533
所以这个时候

597
00:17:08,533 --> 00:17:10,166
我们更多的是一个computer  bound

598
00:17:10,166 --> 00:17:11,166
我们的算力

599
00:17:11,166 --> 00:17:14,366
可能对于一个后面的不是非常的核心

600
00:17:14,366 --> 00:17:14,850
因此

601
00:17:14,850 --> 00:17:16,933
我们对于不同的芯片的产品

602
00:17:16,933 --> 00:17:19,250
或者我们推理的芯片和训练芯片

603
00:17:19,250 --> 00:17:20,500
可能完全不一样

604
00:17:20,500 --> 00:17:23,050
他们两个之间的性价比是不同

605
00:17:23,700 --> 00:17:24,216
另外的话

606
00:17:24,216 --> 00:17:26,533
这篇文章也对一个1K的一个序列

607
00:17:26,533 --> 00:17:28,450
长度做一个简单的分解了

608
00:17:28,450 --> 00:17:29,250
也就是attention

609
00:17:29,250 --> 00:17:30,250
还有FFN

610
00:17:30,250 --> 00:17:31,850
相关的一些处理了

611
00:17:31,850 --> 00:17:33,100
当然还有postproj

612
00:17:33,100 --> 00:17:35,933
还有preрrоj就是里面的归一化的内容

613
00:17:35,933 --> 00:17:37,566
里面做了一些详细的分析

614
00:17:37,566 --> 00:17:38,500
有兴趣的小伙伴

615
00:17:38,500 --> 00:17:40,533
也可以去看一看对应的文章

616
00:17:42,133 --> 00:17:43,050
我们的今天视频

617
00:17:43,050 --> 00:17:44,133
简单的做一个总结

618
00:17:44,133 --> 00:17:45,850
因为整个大模型的推理的需求

619
00:17:45,850 --> 00:17:46,766
不断的增长

620
00:17:46,766 --> 00:17:48,050
然后我们需要去分析

621
00:17:48,050 --> 00:17:50,333
整个大语言模型里面的推理的过

622
00:17:50,333 --> 00:17:51,533
程推理过程实际上

623
00:17:51,533 --> 00:17:53,216
分为profill跟decode阶段

624
00:17:53,216 --> 00:17:54,700
为了解决decode阶段

625
00:17:54,700 --> 00:17:56,133
一个计算慢的问题

626
00:17:56,133 --> 00:17:57,333
和串行慢的问题

627
00:17:57,333 --> 00:17:59,250
我们引入了一个kv cache

628
00:17:59,250 --> 00:18:00,366
一个计算的原理和算法

629
00:18:00,366 --> 00:18:02,733
那kv cache的原点算法

630
00:18:02,733 --> 00:18:03,416
其实会

631
00:18:03,416 --> 00:18:05,700
遇到一个比较大的一个瓶颈

632
00:18:05,700 --> 00:18:06,966
也是在decode的阶段

633
00:18:06,966 --> 00:18:08,166
对于我们的memory Bound

634
00:18:08,166 --> 00:18:10,166
其实存在很大的需求

635
00:18:10,166 --> 00:18:12,650
所以我们会去分析瓶颈的问题

636
00:18:12,733 --> 00:18:13,250
然后也大家

637
00:18:13,250 --> 00:18:15,250
大家怎么简单的去看了一下

638
00:18:15,250 --> 00:18:16,216
那有兴趣小伙伴

639
00:18:16,216 --> 00:18:18,300
也可以深入到对应的热门去看一下

640
00:18:18,300 --> 00:18:20,166
里面到底出现哪些瓶颈

641
00:18:20,166 --> 00:18:22,366
应该用哪些新的算法去解决

642
00:18:22,366 --> 00:18:23,100
今天的内容

643
00:18:23,100 --> 00:18:23,816
就到这里为止

644
00:18:23,816 --> 00:18:24,250
谢谢各位

645
00:18:24,250 --> 00:18:25,050
拜了个拜

